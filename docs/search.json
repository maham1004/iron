[
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "STA 9750 — Mini-Project 01",
    "section": "",
    "text": "Code\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\n# Install if missing\nif(!requireNamespace(\"dplyr\", quietly = TRUE)) install.packages(\"dplyr\")\nif(!requireNamespace(\"DT\", quietly = TRUE)) install.packages(\"DT\")\n\n# Load packages\nlibrary(dplyr)\nlibrary(DT)\n\n# install if missing, then load\nif(!requireNamespace(\"DT\", quietly = TRUE)) install.packages(\"DT\")\nlibrary(DT)\npackageVersion(\"DT\")\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(readr)\nlibrary(dplyr)\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\n\n# Convert \"N/A\" to NA in the season_title column\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 %&gt;%\n  mutate(season_title = if_else(season_title == \"N/A\", NA_character_, season_title))\n\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME, na = \"N/A\", show_col_types = FALSE)\n  \n# Install and load the DT package\n\nlibrary(DT)\n#Make sure DT is loaded in setup chunk (library(DT))\n#Show first 20 rows as an interactive table in the HTML output\nGLOBAL_TOP_10 |&gt;\n  head(n = 20) |&gt;\n  datatable(\n    options = list(searching = FALSE, info = FALSE),\n    rownames = FALSE\n  )\n\n\n\n\nCode\nlibrary(stringr)\nformat_titles &lt;- function(df){\n    colnames(df) &lt;- str_replace_all(colnames(df), \"_\", \" \") |&gt; str_to_title()\n    df\n}\n\nGLOBAL_TOP_10 |&gt; \n    format_titles() |&gt;\n   head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\nCode\nGLOBAL_TOP_10 |&gt; \n  select(-season_title) |&gt;\n  format_titles() |&gt;\n   head(n=20) |&gt;\n   datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n   formatRound(c('Weekly Hours Viewed', 'Weekly Views'))"
  },
  {
    "objectID": "tester presentation.html#beef-bowl",
    "href": "tester presentation.html#beef-bowl",
    "title": "Homemade Eats",
    "section": "Beef Bowl",
    "text": "Beef Bowl\n\n\nHot Honey\nground beef\nsweet potato\navocado"
  },
  {
    "objectID": "tester presentation.html#going-to-sleep",
    "href": "tester presentation.html#going-to-sleep",
    "title": "Homemade Eats",
    "section": "Going to sleep",
    "text": "Going to sleep\n\nGet in bed\nCount sheep"
  },
  {
    "objectID": "tester1.html",
    "href": "tester1.html",
    "title": "tester",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "tester1.html#quarto",
    "href": "tester1.html#quarto",
    "title": "tester",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "tester1.html#running-code",
    "href": "tester1.html#running-code",
    "title": "tester",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi I’m Maham",
    "section": "",
    "text": "Me at Niagara Falls this past summer\n\nHello and welcome to my website! My name is Maham Hassan.\nI am a student at Baruch College working toward earning my Masters degree in Statistics. Here you will find some projects I have completed.\nPrior to this I worked in public finance in the fixed income space as a municipal advisor. I did my undergrad in Math and Economics from Bryn Mawr College in Philadelphia and graduated in 2022. I’m looking to build my technical skills and learn how to work with data sets.\n\n\n\n\n\n\nLast Updated: Friday 09 26, 2025 at 10:33AM"
  },
  {
    "objectID": "mp01.html#total-hours-viewed",
    "href": "mp01.html#total-hours-viewed",
    "title": "STA 9750 — Mini-Project 01",
    "section": "Total hours viewed",
    "text": "Total hours viewed\n\n\nCode\nstranger_things_viewership_hours &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarize(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\nstranger_things_viewership_hours\n\n\n# A tibble: 1 × 1\n  total_hours\n        &lt;dbl&gt;\n1  2967980000"
  },
  {
    "objectID": "mp01.html#number-of-countires-it-charted-in",
    "href": "mp01.html#number-of-countires-it-charted-in",
    "title": "STA 9750 — Mini-Project 01",
    "section": "Number of countires it charted in",
    "text": "Number of countires it charted in\n\n\nCode\nstranger_things_countries &lt;- COUNTRY_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\")\n\ncountries_charted &lt;- stranger_things_countries |&gt;\n  group_by(show_title) |&gt;\n  summarize(num_countries = n_distinct(country_name), .groups = \"drop\")\n\ncountries_charted\n\n\n# A tibble: 1 × 2\n  show_title      num_countries\n  &lt;chr&gt;                   &lt;int&gt;\n1 Stranger Things            93"
  },
  {
    "objectID": "mp01.html#number-of-weeks-in-top-10",
    "href": "mp01.html#number-of-weeks-in-top-10",
    "title": "STA 9750 — Mini-Project 01",
    "section": "Number of weeks in top 10",
    "text": "Number of weeks in top 10\n\n\nCode\n# Filter for Stranger Things cummulative weeks in top 10 \nstranger_things_total_weeks &lt;- COUNTRY_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarize(total_weeks_in_top_10 = sum(cumulative_weeks_in_top_10, na.rm = TRUE))\n\nstranger_things_total_weeks\n\n\n# A tibble: 1 × 1\n  total_weeks_in_top_10\n                  &lt;dbl&gt;\n1                 18853\n\n\nCode\nstranger_things_total_weeks &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarize(total_weeks_in_top_10 = sum(cumulative_weeks_in_top_10, na.rm = TRUE))\n\nstranger_things_total_weeks\n\n\n# A tibble: 1 × 1\n  total_weeks_in_top_10\n                  &lt;dbl&gt;\n1                   366"
  },
  {
    "objectID": "mp01.html#netflix-india-soars-hindi-films-dominate-global-top-10",
    "href": "mp01.html#netflix-india-soars-hindi-films-dominate-global-top-10",
    "title": "STA 9750 — Mini-Project 01",
    "section": "Netflix India Soars: Hindi Films Dominate Global Top 10",
    "text": "Netflix India Soars: Hindi Films Dominate Global Top 10\nNetflix is witnessing remarkable growth in India, with Hindi-language films consistently making waves on the global stage. Analysis of the platform’s top titles in February, April, June, and August 2025 shows that at least four Hindi films each month captured a spot in the global top 10 charts.\nThe total viewership for these hits is impressive. In June 2025, Hindi films collectively amassed 165.4 million hours of global viewing, while April saw 106.9 million hours. February and August tallied 64.3 million and 41.7 million hours, respectively. This corresponds to an estimated 6,457,361 Netflix users in India engaging with these films, underscoring India’s influence on global entertainment.\nKey highlights from the top performers include:\nPushpa 2: The Rise (Feb): nearly 9.5 million estimated views\nDhoom Dhaam (Feb): 9.9 million estimated views\nJaat (Jun): 9.4 million estimated views\nThese figures demonstrate the widespread appeal of Hindi films and provide a meaningful estimate of Netflix’s growing consumer base in India. With multiple top-performing Hindi films each month, Netflix is positioned to continue expanding its reach and engagement in one of the world’s largest streaming markets.\n\n\nCode\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\n\nmonths_to_do &lt;- list(\n  feb2025 = list(year = 2025, month = 2, label = \"Feb 2025\"),\n  apr2025 = list(year = 2025, month = 4, label = \"Apr 2025\"),\n  jun2025 = list(year = 2025, month = 6, label = \"Jun 2025\"),\n  aug2025 = list(year = 2025, month = 8, label = \"Aug 2025\")\n)\n\nprocess_month &lt;- function(year_i, month_i, label_i) {\n  # India Top-10 titles for the month\n  india_titles &lt;- COUNTRY_TOP_10 |&gt;\n    filter(country_name == \"India\",\n           year(week) == year_i,\n           month(week) == month_i,\n           weekly_rank &lt;= 10) |&gt;\n    select(show_title) |&gt;\n    distinct()\n  \n  # Global rows for same month\n  global_month &lt;- GLOBAL_TOP_10 |&gt;\n    filter(year(week) == year_i, month(week) == month_i)\n  \n  # Keep only global rows that match India titles\n  matched &lt;- merge(india_titles, global_month, by = \"show_title\", all.x = FALSE, all.y = FALSE)\n  \n  # Keep non-English films only\n  matched_noneng &lt;- matched |&gt;\n    filter(category == \"Films (Non-English)\")\n  \n  if (nrow(matched_noneng) == 0) {\n    return(tibble(\n      month = character(0),\n      show_title = character(0),\n      cumulative_weeks_in_top_10 = numeric(0),\n      month_hours = numeric(0),\n      runtime = numeric(0),\n      est_views = numeric(0)\n    ))\n  }\n  \n  # Summarize per title, I also filtered out movies with runtimes shorter than 1.8\n  #  based on cultural contaxt, most hindi\n  #  films have runtimes over 1.75, whereas most westerns ones do not.\n  matched_noneng |&gt;\n    group_by(show_title) |&gt;\n    summarize(\n      cumulative_weeks_in_top_10 = max(cumulative_weeks_in_top_10, na.rm = TRUE),\n      month_hours = sum(weekly_hours_viewed, na.rm = TRUE),\n      runtime = if (all(is.na(runtime))) NA_real_ else first(runtime[!is.na(runtime)]),\n      .groups = \"drop\"\n    ) |&gt;\n    filter(!is.na(runtime), runtime &gt;= 1.8) |&gt;    \n    mutate(\n      est_views = round(month_hours / runtime),\n      month = label_i\n    ) |&gt;\n    select(month, show_title, cumulative_weeks_in_top_10, month_hours, runtime, est_views) |&gt;\n    arrange(desc(month_hours))\n}\n\n# run for each month and combine\nresults_list &lt;- lapply(months_to_do, function(m) process_month(m$year, m$month, m$label))\ncombined_tbl &lt;- bind_rows(results_list)\n\n# show combined table\nprint(combined_tbl, n = Inf)\n\n\n# A tibble: 25 × 6\n   month    show_title      cumulative_weeks_in_…¹ month_hours runtime est_views\n   &lt;chr&gt;    &lt;chr&gt;                            &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Feb 2025 Pushpa 2: The …                      2    35400000    3.73   9482228\n 2 Feb 2025 Dhoom Dhaam                          2    18000000    1.82   9908075\n 3 Feb 2025 Daaku Maharaaj                       1     5700000    2.38   2391642\n 4 Feb 2025 Kadhalikka Ner…                      1     5200000    2.33   2228603\n 5 Apr 2025 Bullet Train E…                      1    28000000    2.28  12262953\n 6 Apr 2025 Court: State v…                      3    16100000    2.48   6483309\n 7 Apr 2025 Jewel Thief - …                      1    15400000    1.97   7830376\n 8 Apr 2025 Deva                                 3    15200000    2.58   5883947\n 9 Apr 2025 Chhaava                              2    14700000    2.63   5582349\n10 Apr 2025 TEST                                 2    12500000    2.43   5137057\n11 Apr 2025 Dragon                               3     2700000    2.57   1051934\n12 Apr 2025 Officer on Duty                      3     2300000    2.23   1029866\n13 Jun 2025 A Widow's Game                       5    83500000    2.03  41066247\n14 Jun 2025 Jaat                                 3    23500000    2.5    9400000\n15 Jun 2025 HIT: The Third…                      2    17900000    2.58   6929122\n16 Jun 2025 Sikandar                             2    14900000    2.23   6671741\n17 Jun 2025 Raid 2                               1    12900000    2.28   5649718\n18 Jun 2025 Retro                                2    12700000    2.72   4674789\n19 Aug 2025 Maa                                  2    12100000    2.22   5458565\n20 Aug 2025 Maareesan                            2    10600000    2.5    4240000\n21 Aug 2025 Kingdom                              1     7400000    2.53   2921091\n22 Aug 2025 Tehran                               1     4200000    1.93   2172451\n23 Aug 2025 Metro... In Di…                      1     3200000    2.67   1199985\n24 Aug 2025 Thammudu                             1     2500000    2.52    993364\n25 Aug 2025 Oho Enthan Baby                      1     1700000    2.17    784603\n# ℹ abbreviated name: ¹​cumulative_weeks_in_top_10\n\n\nCode\n# Estimate the number of Indian Netflix users based on the est_views column\nestimated_indian_users &lt;- combined_tbl |&gt;\n  summarize(avg_est_views = mean(est_views, na.rm = TRUE)) |&gt;\n  pull(avg_est_views)\n\n# Display the estimate\nestimated_indian_users\n\n\n[1] 6457361\n\n\nCode\n# monthly totals in hours and converted to thousands for plotting\nmonthly_totals &lt;- combined_tbl |&gt;\n  group_by(month) |&gt;\n  summarize(total_month_hours = sum(month_hours, na.rm = TRUE), .groups = \"drop\") |&gt;\n  # preserve desired month order\n  arrange(match(month, c(\"Feb 2025\", \"Apr 2025\", \"Jun 2025\", \"Aug 2025\"))) |&gt;\n  mutate(total_month_hours_k = total_month_hours / 10000)\n\nmonthly_totals\n\n\n# A tibble: 4 × 3\n  month    total_month_hours total_month_hours_k\n  &lt;chr&gt;                &lt;dbl&gt;               &lt;dbl&gt;\n1 Feb 2025          64300000                6430\n2 Apr 2025         106900000               10690\n3 Jun 2025         165400000               16540\n4 Aug 2025          41700000                4170\n\n\nCode\n# line plot: y axis in tenthousands of hours\nggplot(monthly_totals, aes(x = factor(month, levels = c(\"Feb 2025\", \"Apr 2025\", \"Jun 2025\", \"Aug 2025\")),\n                           y = total_month_hours_k, group = 1)) +\n  geom_line() +\n  geom_point(size = 3) +\n  labs(\n    title = \"Total viewing hours (non-English films that charted in India)\",\n    subtitle = \"Feb / Apr / Jun / Aug 2025 — totals shown in ten-thousands of hours\",\n    x = \"Month (2025)\",\n    y = \"Total viewing hours (tenthousands)\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "mp01.html#stranger-things-returns-season-5-set-to-thrill-fans-worldwide",
    "href": "mp01.html#stranger-things-returns-season-5-set-to-thrill-fans-worldwide",
    "title": "STA 9750 — Mini-Project 01",
    "section": "Stranger Things Returns: Season 5 Set to Thrill Fans Worldwide",
    "text": "Stranger Things Returns: Season 5 Set to Thrill Fans Worldwide\nNetflix is gearing up for the release of the fifth and final season of its global phenomenon, Stranger Things, at the end of 2025. The show’s previous four seasons have left an indelible mark on viewers, charting across 93 countries and amassing 366 cumulative weeks in the global Top 10.\nThe total viewership of the series is staggering, with over 2.97 billion hours watched worldwide. This incredible engagement highlights not only the loyalty of the fan base but also the series’ ability to capture audiences across multiple continents.\nCompared to other English-language TV shows, Stranger Things stands out for its enduring popularity and international appeal. The combination of long-standing dominance in top charts and massive global viewership underscores why fans are eagerly awaiting the final season.\nWith Season 5, Netflix is poised to deliver a conclusion worthy of one of its most iconic original series, keeping millions of viewers around the world glued to their screens.\n\n\nCode\n## Number of weeks in top 10\n# Filter for Stranger Things cummulative weeks in top 10 \nstranger_things_total_weeks &lt;- COUNTRY_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarize(total_weeks_in_top_10 = sum(cumulative_weeks_in_top_10, na.rm = TRUE))\n\nstranger_things_total_weeks\n\n\n# A tibble: 1 × 1\n  total_weeks_in_top_10\n                  &lt;dbl&gt;\n1                 18853\n\n\nCode\nstranger_things_total_weeks &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarize(total_weeks_in_top_10 = sum(cumulative_weeks_in_top_10, na.rm = TRUE))\n\nstranger_things_total_weeks\n\n\n# A tibble: 1 × 1\n  total_weeks_in_top_10\n                  &lt;dbl&gt;\n1                   366\n\n\n\n\nCode\n## Total hours viewed\nstranger_things_viewership_hours &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarize(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\nstranger_things_viewership_hours\n\n\n# A tibble: 1 × 1\n  total_hours\n        &lt;dbl&gt;\n1  2967980000\n\n\n\n\nCode\n## Number of countires it charted in\nstranger_things_countries &lt;- COUNTRY_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\")\n\ncountries_charted &lt;- stranger_things_countries |&gt;\n  group_by(show_title) |&gt;\n  summarize(num_countries = n_distinct(country_name), .groups = \"drop\")\n\ncountries_charted\n\n\n# A tibble: 1 × 2\n  show_title      num_countries\n  &lt;chr&gt;                   &lt;int&gt;\n1 Stranger Things            93"
  },
  {
    "objectID": "mp01.html#netflix-india-soars-as-hindi-films-take-global-top-10-by-storm",
    "href": "mp01.html#netflix-india-soars-as-hindi-films-take-global-top-10-by-storm",
    "title": "STA 9750 — Mini-Project 01",
    "section": "Netflix India Soars As Hindi Films Take Global Top 10 By Storm",
    "text": "Netflix India Soars As Hindi Films Take Global Top 10 By Storm\nNetflix is witnessing remarkable growth in India, with Hindi-language films making big waves on the global stage. Analysis of the platform’s top titles in February, April, June, and August 2025 shows that at least four Hindi films each month captured a spot in the global top 10 charts.\nThe total viewership for these hits is impressive. In June 2025, Hindi films collectively amassed 165.4 million hours of global viewing, while April saw 106.9 million hours. February and August tallied 64.3 million and 41.7 million hours, respectively. This corresponds to an estimatedd 6,457,361 Netflix users in India engaging with these films, underscoring India’s influence on global entertainment.\nKey highlights from the top performers include:\nPushpa 2: The Rise (Feb): nearly 9.5 million estimated views\nDhoom Dhaam (Feb): 9.9 million estimated views\nJaat (Jun): 9.4 million estimated views\nThese figures demonstrate the widespread appeal of Hindi films and provide a meaningful estimate of Netflix’s growing consumer base in India. With multiple top-performing Hindi films each month, Netflix is positioned to continue expanding its engagement in one of the world’s largest streaming markets.\n\n\nCode\n#We will take the top ten india data from the country dataset and \n#use a few different months in 2025. this data will only include \n#non-english top ten titles in india to assume that those are hindi. Now \n#we will use this data to search up all these titles in the global #dataset. \n#This will allow us to make a table that includes the #columns cummulative weeks in top ten, \n#weekly_hours_viewed, runtime,\n#and weekly hours viewed divided by runtime.\n#We will also take an #average of the times \n#watched column so that we can find the #estimated customer base\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\n\nmonths_to_do &lt;- list(\n  feb2025 = list(year = 2025, month = 2, label = \"Feb 2025\"),\n  apr2025 = list(year = 2025, month = 4, label = \"Apr 2025\"),\n  jun2025 = list(year = 2025, month = 6, label = \"Jun 2025\"),\n  aug2025 = list(year = 2025, month = 8, label = \"Aug 2025\")\n)\n\nprocess_month &lt;- function(year_i, month_i, label_i) {\n  # India Top-10 titles for the month\n  india_titles &lt;- COUNTRY_TOP_10 |&gt;\n    filter(country_name == \"India\",\n           year(week) == year_i,\n           month(week) == month_i,\n           weekly_rank &lt;= 10) |&gt;\n    select(show_title) |&gt;\n    distinct()\n  \n  # Global rows for same month\n  global_month &lt;- GLOBAL_TOP_10 |&gt;\n    filter(year(week) == year_i, month(week) == month_i)\n  \n  # Keep only global rows that match India titles\n  matched &lt;- merge(india_titles, global_month, by = \"show_title\", all.x = FALSE, all.y = FALSE)\n  \n  # Keep non-English films only\n  matched_noneng &lt;- matched |&gt;\n    filter(category == \"Films (Non-English)\")\n  \n  if (nrow(matched_noneng) == 0) {\n    return(tibble(\n      month = character(0),\n      show_title = character(0),\n      cumulative_weeks_in_top_10 = numeric(0),\n      month_hours = numeric(0),\n      runtime = numeric(0),\n      est_views = numeric(0)\n    ))\n  }\n  \n# Summarize per title, I assumed that the non-english films would be hindi and\n# I also assumed and filtered out movies with runtimes shorter than 1.8\n#  based on cultural contextt, most hindi\n#  films have runtimes over 1.75, whereas most westerns ones do not.\n  matched_noneng |&gt;\n    group_by(show_title) |&gt;\n    summarize(\n      cumulative_weeks_in_top_10 = max(cumulative_weeks_in_top_10, na.rm = TRUE),\n      month_hours = sum(weekly_hours_viewed, na.rm = TRUE),\n      runtime = if (all(is.na(runtime))) NA_real_ else first(runtime[!is.na(runtime)]),\n      .groups = \"drop\"\n    ) |&gt;\n    filter(!is.na(runtime), runtime &gt;= 1.8) |&gt;    \n    mutate(\n      est_views = round(month_hours / runtime),\n      month = label_i\n    ) |&gt;\n    select(month, show_title, cumulative_weeks_in_top_10, month_hours, runtime, est_views) |&gt;\n    arrange(desc(month_hours))\n}\n\n# run for each month and combine\nresults_list &lt;- lapply(months_to_do, function(m) process_month(m$year, m$month, m$label))\ncombined_tbl &lt;- bind_rows(results_list)\n\n\n\nTable of Hindi films that made the global top ten list in various months in 2025:\n\n\nCode\n# show combined table\nprint(combined_tbl, n = Inf)\n\n\n# A tibble: 25 × 6\n   month    show_title      cumulative_weeks_in_…¹ month_hours runtime est_views\n   &lt;chr&gt;    &lt;chr&gt;                            &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Feb 2025 Pushpa 2: The …                      2    35400000    3.73   9482228\n 2 Feb 2025 Dhoom Dhaam                          2    18000000    1.82   9908075\n 3 Feb 2025 Daaku Maharaaj                       1     5700000    2.38   2391642\n 4 Feb 2025 Kadhalikka Ner…                      1     5200000    2.33   2228603\n 5 Apr 2025 Bullet Train E…                      1    28000000    2.28  12262953\n 6 Apr 2025 Court: State v…                      3    16100000    2.48   6483309\n 7 Apr 2025 Jewel Thief - …                      1    15400000    1.97   7830376\n 8 Apr 2025 Deva                                 3    15200000    2.58   5883947\n 9 Apr 2025 Chhaava                              2    14700000    2.63   5582349\n10 Apr 2025 TEST                                 2    12500000    2.43   5137057\n11 Apr 2025 Dragon                               3     2700000    2.57   1051934\n12 Apr 2025 Officer on Duty                      3     2300000    2.23   1029866\n13 Jun 2025 A Widow's Game                       5    83500000    2.03  41066247\n14 Jun 2025 Jaat                                 3    23500000    2.5    9400000\n15 Jun 2025 HIT: The Third…                      2    17900000    2.58   6929122\n16 Jun 2025 Sikandar                             2    14900000    2.23   6671741\n17 Jun 2025 Raid 2                               1    12900000    2.28   5649718\n18 Jun 2025 Retro                                2    12700000    2.72   4674789\n19 Aug 2025 Maa                                  2    12100000    2.22   5458565\n20 Aug 2025 Maareesan                            2    10600000    2.5    4240000\n21 Aug 2025 Kingdom                              1     7400000    2.53   2921091\n22 Aug 2025 Tehran                               1     4200000    1.93   2172451\n23 Aug 2025 Metro... In Di…                      1     3200000    2.67   1199985\n24 Aug 2025 Thammudu                             1     2500000    2.52    993364\n25 Aug 2025 Oho Enthan Baby                      1     1700000    2.17    784603\n# ℹ abbreviated name: ¹​cumulative_weeks_in_top_10\n\n\nEstimate of the number of Indian Netflix users based on the est_views column\n\n\nCode\nestimated_indian_users &lt;- combined_tbl |&gt;\n  summarize(avg_est_views = mean(est_views, na.rm = TRUE)) |&gt;\n  pull(avg_est_views)\n\n# Display the estimate\nestimated_indian_users\n\n\n[1] 6457361\n\n\n\n\nCode\n# Monthly totals in hours and converted to thousands for plotting\nmonthly_totals &lt;- combined_tbl |&gt;\n  group_by(month) |&gt;\n  summarize(total_month_hours = sum(month_hours, na.rm = TRUE), .groups = \"drop\") |&gt;\n  # Preserve desired month order\n  arrange(match(month, c(\"Feb 2025\", \"Apr 2025\", \"Jun 2025\", \"Aug 2025\"))) |&gt;\n  mutate(total_month_hours_k = total_month_hours / 10000)\n\nmonthly_totals\n\n\n# line plot: y axis in tenthousands of hours\nggplot(monthly_totals, aes(x = factor(month, levels = c(\"Feb 2025\", \"Apr 2025\", \"Jun 2025\", \"Aug 2025\")),\n                           y = total_month_hours_k, group = 1)) +\n  geom_line() +\n  geom_point(size = 3) +\n  labs(\n    title = \"Total viewing hours (Non-English films that charted in India)\",\n    subtitle = \"Feb / Apr / Jun / Aug 2025 — totals shown in ten-thousands of hours\",\n    x = \"Month (2025)\",\n    y = \"Total viewing hours (tenthousands)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n#No increasing correlation is shown."
  },
  {
    "objectID": "mp01.html#stranger-things-season-5-set-to-shock-the-world-a-netflix-phenomenon-like-no-other",
    "href": "mp01.html#stranger-things-season-5-set-to-shock-the-world-a-netflix-phenomenon-like-no-other",
    "title": "STA 9750 — Mini-Project 01",
    "section": "Stranger Things Season 5 Set to Shock the World: A Netflix Phenomenon Like No Other!",
    "text": "Stranger Things Season 5 Set to Shock the World: A Netflix Phenomenon Like No Other!\nNetflix is gearing up for the release of the final season of its hit, Stranger Things, at the end of 2025. The show’s previous four seasons have left an indelible mark on viewers, charting across 93 countries and amassing 366 cumulative weeks in the global Top 10.\nThe total viewership of the series is staggering, with over 2.97 billion hours watched worldwide. This incredible engagement highlights the loyalty of the fan base and the show’s ability to capture audiences across multiple continents.\nStranger Things continues to captivate audiences worldwide, boasting nearly 3 billion hours of total viewership. Yet it shares the spotlight with Netflix’s international sensation Squid Game, which has eclipsed 5 billion hours watched. This remarkable performance from both shows highlights the global appeal of Netflix’s hits, and with its long standing dominance on top charts, anticipation is soaring for the fifth and final season of Stranger Things.\nWith Season 5, Netflix is expected to deliver a conclusion worthy of one of its most iconic original series, keeping millions around the world glued to their screens.\n\n\nCode\n## Number of weeks in top 10\n# Filter for Stranger Things cummulative weeks in top 10 \nstranger_things_total_weeks &lt;- COUNTRY_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarize(total_weeks_in_top_10 = sum(cumulative_weeks_in_top_10, na.rm = TRUE))\n\nstranger_things_total_weeks\n\n\nstranger_things_total_weeks &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarize(total_weeks_in_top_10 = sum(cumulative_weeks_in_top_10, na.rm = TRUE))\n\nstranger_things_total_weeks\n\n\n\n\nCode\n## Total hours viewed\nstranger_things_viewership_hours &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarize(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\nstranger_things_viewership_hours\n\n\n\n\nCode\n## Number of countires it charted in\nstranger_things_countries &lt;- COUNTRY_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\")\n\ncountries_charted &lt;- stranger_things_countries |&gt;\n  group_by(show_title) |&gt;\n  summarize(num_countries = n_distinct(country_name), .groups = \"drop\")\n\ncountries_charted"
  },
  {
    "objectID": "mp01.html#kpop-demon-hunters-shatters-records-to-become-netflixs-most-watched-film-in-history",
    "href": "mp01.html#kpop-demon-hunters-shatters-records-to-become-netflixs-most-watched-film-in-history",
    "title": "STA 9750 — Mini-Project 01",
    "section": "KPop Demon Hunters Shatters Records to Become Netflix’s Most Watched Film in History",
    "text": "KPop Demon Hunters Shatters Records to Become Netflix’s Most Watched Film in History\nNetflix has a new global champion in the ring. KPop Demon Hunters has officially claimed the crown as Netflix’s most watched film of all time, igniting a worldwide frenzy.\nThe animated hit debuted at #2 on Netflix’s Global Top 10 before skyrocketing to the #1 position, where it has reigned supreme for an astounding seven weeks. In total, the film has dominated the Top 10 for 13 weeks, captivating audiences across every continent.\nThe numbers are nothing short of astounding: KPop Demon Hunters has amassed about 314 million views. From Seoul’s neon streets to screens in New York, Paris, and São Paulo, this vibrant and unoque film concept has set the world ablaze.\nCritics and fans alike are calling it a global pop culture phenomenon, blending the heart-pounding energy of K-pop with the cinematic scale of an animated epic. The film’s runaway success underscores Netflix’s growing influence in championing Asian stories that connect universally.\nWith KPop Demon Hunters, Netflix has proven once again that global storytelling knows no borders.\n\n\nCode\n#Getting data for Kpop Demon Hunter Press release\n# Filter for KPop Demon Hunters in the global dataset\n\n\nkpop_demon_hunters &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"KPop Demon Hunters\")\n\n# Summarize total hours, average runtime, cumulative weeks, and weeks at #1\nkpop_summary &lt;- kpop_demon_hunters |&gt;\n  summarize(\n    total_hours = sum(weekly_hours_viewed, na.rm = TRUE),\n    runtime = mean(runtime, na.rm = TRUE),\n    total_weeks_in_top_10 = max(cumulative_weeks_in_top_10, na.rm = TRUE),\n    weeks_at_number_1 = sum(weekly_rank == 1, na.rm = TRUE)\n  )\n\n# Calculate estimated total views \nkpop_summary &lt;- kpop_summary |&gt;\n  mutate(estimated_views = total_hours / runtime)\n\n# Find its debut rank \nkpop_debut &lt;- kpop_demon_hunters |&gt;\n  arrange(week) |&gt;\n  slice_head(n = 1)\n\n# Combine debut rank with summary table\nkpop_table &lt;- kpop_summary |&gt;\n  mutate(debut_rank = kpop_debut$weekly_rank)\n\n# Show final table\nkpop_table\n\n#Graph of Kpop Demon hunters after debut and climbing the ranks\nlibrary(ggplot2)\n\nggplot(kpop_demon_hunters, aes(x = week, y = weekly_rank)) +\n  geom_line(color = \"#e60073\", size = 1.2) +\n  geom_point(aes(color = weekly_rank == 1), size = 3) +\n  scale_y_reverse(breaks = 1:10) +  # rank 1 at top\n  labs(\n    title = \"KPop Demon Hunters: Climbing to #1 on Netflix\",\n    subtitle = \"Debuted at #2 and held the #1 spot for 7 different weeks\",\n    x = \"Week\",\n    y = \"Global Weekly Rank\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\", color = \"#e60073\"),\n    plot.subtitle = element_text(size = 11)\n  )"
  },
  {
    "objectID": "mp01.html#column-names-for-global-data",
    "href": "mp01.html#column-names-for-global-data",
    "title": "STA 9750 — Mini-Project 01",
    "section": "Column Names for Global Data",
    "text": "Column Names for Global Data\n\n\nCode\ncolnames(GLOBAL_TOP_10)\n\n\n[1] \"week\"                       \"category\"                  \n[3] \"weekly_rank\"                \"show_title\"                \n[5] \"season_title\"               \"weekly_hours_viewed\"       \n[7] \"runtime\"                    \"weekly_views\"              \n[9] \"cumulative_weeks_in_top_10\""
  },
  {
    "objectID": "mp01.html#column-cames-for-country-data",
    "href": "mp01.html#column-cames-for-country-data",
    "title": "STA 9750 — Mini-Project 01",
    "section": "Column Cames for Country Data",
    "text": "Column Cames for Country Data\n\n\nCode\ncolnames(COUNTRY_TOP_10)\n\n\n[1] \"country_name\"               \"country_iso2\"              \n[3] \"week\"                       \"category\"                  \n[5] \"weekly_rank\"                \"show_title\"                \n[7] \"season_title\"               \"cumulative_weeks_in_top_10\""
  },
  {
    "objectID": "mp01.html#column-names-for-country-data",
    "href": "mp01.html#column-names-for-country-data",
    "title": "STA 9750 — Mini-Project 01",
    "section": "Column Names for Country Data",
    "text": "Column Names for Country Data\n\n\nCode\ncolnames(COUNTRY_TOP_10)\n\n\n[1] \"country_name\"               \"country_iso2\"              \n[3] \"week\"                       \"category\"                  \n[5] \"weekly_rank\"                \"show_title\"                \n[7] \"season_title\"               \"cumulative_weeks_in_top_10\""
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "STA 9750 — Mini-Project 02",
    "section": "",
    "text": "Housing affordability has become a huge challenge facing cities across the country, the supply of housing just cannot keep up with the demand. A lot of the debate comes down to two clashing philosophies: YIMBY (“Yes In My Backyard”), which argues that cities should make it easier to build housing, and NIMBY (“Not In My Backyard”), which pushes back against new development out of fear that it will change neighborhood character, lead to gentrification, or hurt property values. It’s a controversial topic because both sides claim to be protecting their communities.\nIn this project, we will dig into Census, ACS and BLS data to see which metro areas are embracing the YIMBY approach and which ones are effectively blocking growth. We will build rent burden and housing growth metrics, track trends across hundreds of CBSAs, and identify a handful of metros that stand out as YIMBY success stories.\n\n\nCode\ntidycensus::census_api_key(\"aa14982ecd4bb502eb657c121f3d02e9b1c45cee\", install = FALSE)\n\ngetwd()\nlist.files(\"data\", recursive = TRUE)\n\n\nif(!dir.exists(file.path(\"data\", \"mp02\"))){\n    dir.create(file.path(\"data\", \"mp02\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nlibrary &lt;- function(pkg){\n    ## Mask base::library() to automatically install packages if needed\n    ## Masking is important here so downlit picks up packages and links\n    ## to documentation\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE, quietly=TRUE))\n}\n\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(readxl)\nlibrary(tidycensus)\n\nget_acs_all_years &lt;- function(variable, geography=\"cbsa\",\n                              start_year=2009, end_year=2023){\n    fname &lt;- glue(\"{variable}_{geography}_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \nif(!file.exists(fname)){\n        YEARS &lt;- seq(start_year, end_year)\n        YEARS &lt;- YEARS[YEARS != 2020] # Drop 2020 - No survey (covid)\n        \n        ALL_DATA &lt;- map(YEARS, function(yy){\n            tidycensus::get_acs(geography, variable, year=yy, survey=\"acs1\") |&gt;\n                mutate(year=yy) |&gt;\n                select(-moe, -variable) |&gt;\n                rename(!!variable := estimate)\n        }) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\n# Household income (12 month)\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;\n    rename(household_income = B19013_001)\n\n# Monthly rent\nRENT &lt;- get_acs_all_years(\"B25064_001\") |&gt;\n    rename(monthly_rent = B25064_001)\n\n# Total population\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;\n    rename(population = B01003_001)\n\n# Total number of households\nHOUSEHOLDS &lt;- get_acs_all_years(\"B11001_001\") |&gt;\n    rename(households = B11001_001)\nget_building_permits &lt;- function(start_year = 2009, end_year = 2023){\n    fname &lt;- glue(\"housing_units_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        HISTORICAL_YEARS &lt;- seq(start_year, 2018)\n        \n        HISTORICAL_DATA &lt;- map(HISTORICAL_YEARS, function(yy){\n            historical_url &lt;- glue(\"https://www.census.gov/construction/bps/txt/tb3u{yy}.txt\")\n                \n            LINES &lt;- readLines(historical_url)[-c(1:11)]\n\n            CBSA_LINES &lt;- str_detect(LINES, \"^[[:digit:]]\")\n            CBSA &lt;- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))\n\n            PERMIT_LINES &lt;- str_detect(str_sub(LINES, 48, 53), \"[[:digit:]]\")\n            PERMITS &lt;- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))\n            \n            data_frame(CBSA = CBSA,\n                       new_housing_units_permitted = PERMITS, \n                       year = yy)\n        }) |&gt; bind_rows()\n        \n        CURRENT_YEARS &lt;- seq(2019, end_year)\n        \n        CURRENT_DATA &lt;- map(CURRENT_YEARS, function(yy){\n            current_url &lt;- glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls\")\n            \n            temp &lt;- tempfile()\n            \n            download.file(current_url, destfile = temp, mode=\"wb\")\n            \n            fallback &lt;- function(.f1, .f2){\n                function(...){\n                    tryCatch(.f1(...), \n                             error=function(e) .f2(...))\n                }\n            }\n            \n            reader &lt;- fallback(read_xlsx, read_xls)\n            \n            reader(temp, skip=5) |&gt;\n                na.omit() |&gt;\n                select(CBSA, Total) |&gt;\n                mutate(year = yy) |&gt;\n                rename(new_housing_units_permitted = Total)\n        }) |&gt; bind_rows()\n        \n        ALL_DATA &lt;- rbind(HISTORICAL_DATA, CURRENT_DATA)\n        \n        write_csv(ALL_DATA, fname)\n        \n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nPERMITS &lt;- get_building_permits()\n\nlibrary(httr2)\nlibrary(rvest)\nget_bls_industry_codes &lt;- function(){\n    fname &lt;- file.path(\"data\", \"mp02\", \"bls_industry_codes.csv\")\n    library(dplyr)\n    library(tidyr)\n    library(readr)\n    \n    if(!file.exists(fname)){\n        \n        resp &lt;- request(\"https://www.bls.gov\") |&gt; \n            req_url_path(\"cew\", \"classifications\", \"industry\", \"industry-titles.htm\") |&gt;\n            req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n            req_error(is_error = \\(resp) FALSE) |&gt;\n            req_perform()\n        \n        resp_check_status(resp)\n        \n        naics_table &lt;- resp_body_html(resp) |&gt;\n            html_element(\"#naics_titles\") |&gt; \n            html_table() |&gt;\n            mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), \"NAICS\"))) |&gt;\n            select(-`Industry Title`) |&gt;\n            mutate(depth = if_else(nchar(Code) &lt;= 5, nchar(Code) - 1, NA)) |&gt;\n            filter(!is.na(depth))\n        \n        # These were looked up manually on bls.gov after finding \n        # they were presented as ranges. Since there are only three\n        # it was easier to manually handle than to special-case everything else\n        naics_missing &lt;- tibble::tribble(\n            ~Code, ~title, ~depth, \n            \"31\", \"Manufacturing\", 1,\n            \"32\", \"Manufacturing\", 1,\n            \"33\", \"Manufacturing\", 1,\n            \"44\", \"Retail\", 1, \n            \"45\", \"Retail\", 1,\n            \"48\", \"Transportation and Warehousing\", 1, \n            \"49\", \"Transportation and Warehousing\", 1\n        )\n        \n        naics_table &lt;- bind_rows(naics_table, naics_missing)\n        \n        naics_table &lt;- naics_table |&gt; \n            filter(depth == 4) |&gt; \n            rename(level4_title=title) |&gt; \n            mutate(level1_code = str_sub(Code, end=2), \n                   level2_code = str_sub(Code, end=3), \n                   level3_code = str_sub(Code, end=4)) |&gt;\n            left_join(naics_table, join_by(level1_code == Code)) |&gt;\n            rename(level1_title=title) |&gt;\n            left_join(naics_table, join_by(level2_code == Code)) |&gt;\n            rename(level2_title=title) |&gt;\n            left_join(naics_table, join_by(level3_code == Code)) |&gt;\n            rename(level3_title=title) |&gt;\n            select(-starts_with(\"depth\")) |&gt;\n            rename(level4_code = Code) |&gt;\n            select(level1_title, level2_title, level3_title, level4_title, \n                   level1_code,  level2_code,  level3_code,  level4_code) |&gt;\n            drop_na() |&gt;\n            mutate(across(contains(\"code\"), as.integer))\n        \n        write_csv(naics_table, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nINDUSTRY_CODES &lt;- get_bls_industry_codes()\nlibrary(httr2)\nlibrary(rvest)\nget_bls_qcew_annual_averages &lt;- function(start_year=2009, end_year=2023){\n    fname &lt;- glue(\"bls_qcew_{start_year}_{end_year}.csv.gz\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    YEARS &lt;- seq(start_year, end_year)\n    YEARS &lt;- YEARS[YEARS != 2020] # Drop Covid year to match ACS\n    \n    if(!file.exists(fname)){\n        ALL_DATA &lt;- map(YEARS, .progress=TRUE, possibly(function(yy){\n            fname_inner &lt;- file.path(\"data\", \"mp02\", glue(\"{yy}_qcew_annual_singlefile.zip\"))\n            \n            if(!file.exists(fname_inner)){\n                request(\"https://www.bls.gov\") |&gt; \n                    req_url_path(\"cew\", \"data\", \"files\", yy, \"csv\",\n                                 glue(\"{yy}_annual_singlefile.zip\")) |&gt;\n                    req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n                    req_retry(max_tries=5) |&gt;\n                    req_perform(fname_inner)\n            }\n            \n            if(file.info(fname_inner)$size &lt; 755e5){\n                warning(sQuote(fname_inner), \"appears corrupted. Please delete and retry this step.\")\n            }\n            \n            read_csv(fname_inner, \n                     show_col_types=FALSE) |&gt; \n                mutate(YEAR = yy) |&gt;\n                select(area_fips, \n                       industry_code, \n                       annual_avg_emplvl, \n                       total_annual_wages, \n                       YEAR) |&gt;\n                filter(nchar(industry_code) &lt;= 5, \n                       str_starts(area_fips, \"C\")) |&gt;\n                filter(str_detect(industry_code, \"-\", negate=TRUE)) |&gt;\n                mutate(FIPS = area_fips, \n                       INDUSTRY = as.integer(industry_code), \n                       EMPLOYMENT = as.integer(annual_avg_emplvl), \n                       TOTAL_WAGES = total_annual_wages) |&gt;\n                select(-area_fips, \n                       -industry_code, \n                       -annual_avg_emplvl, \n                       -total_annual_wages) |&gt;\n                # 10 is a special value: \"all industries\" , so omit\n                filter(INDUSTRY != 10) |&gt; \n                mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)\n        })) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    ALL_DATA &lt;- read_csv(fname, show_col_types=FALSE)\n    \n    ALL_DATA_YEARS &lt;- unique(ALL_DATA$YEAR)\n    \n    YEARS_DIFF &lt;- setdiff(YEARS, ALL_DATA_YEARS)\n    \n    if(length(YEARS_DIFF) &gt; 0){\n        stop(\"Download failed for the following years: \", YEARS_DIFF, \n             \". Please delete intermediate files and try again.\")\n    }\n    \n    ALL_DATA\n}\n\nWAGES &lt;- get_bls_qcew_annual_averages()"
  },
  {
    "objectID": "mp02.html#initial-data-exploration",
    "href": "mp02.html#initial-data-exploration",
    "title": "STA 9750 — Mini-Project 02",
    "section": "Initial Data Exploration",
    "text": "Initial Data Exploration\n\nWhich CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?\n\nThe Houston-Sugarland-Baytown area in Texas has permitted the largest number of housing units from 2010 to 2019 during which they allowed 482,075 units to be built.\n\n\nCode\nlibrary(tidyverse)\n\n# Load the housing data\nhousing &lt;- read_csv(\"data/mp02/housing_units_2009_2023.csv\")\n\n# Check the first few rows and structure\nglimpse(housing)\n\n#| include: false\n# Filter housing data for 2010-2019\nhousing_2010_2019 &lt;- housing |&gt;\n  filter(year &gt;= 2010 & year &lt;= 2019)\n\n# Quick check\nglimpse(housing_2010_2019)\n\n#| include: false\n# Sum new housing units per CBSA for the decade\nhousing_summed &lt;- housing_2010_2019 |&gt;\n  group_by(CBSA) |&gt;\n  summarise(total_units = sum(new_housing_units_permitted, na.rm = TRUE)) |&gt;\n  arrange(desc(total_units))\n\n# Look at top CBSA totals\nhead(housing_summed, 10)\n\n#| echo: true\n# Sum new housing units per CBSA for the decade\nhousing_summed &lt;- housing_2010_2019 |&gt;\n  group_by(CBSA) |&gt;\n  summarise(total_units = sum(new_housing_units_permitted, na.rm = TRUE)) |&gt;\n  arrange(desc(total_units))\n\n# Load CBSA metadata\ncbsa_meta &lt;- read_csv(\"data/mp02/B01003_001_cbsa_2009_2023.csv\")\n\n# Join summed housing with CBSA names using GEOID\nhousing_named &lt;- housing_summed |&gt;\n  left_join(cbsa_meta, by = c(\"CBSA\" = \"GEOID\"))\n\n# Show the CBSA with the largest total units\nhousing_named |&gt; slice(1)\n\n#| echo: true\nlibrary(DT)\n\n# Get the CBSA with the largest total units\ntop_cbsa &lt;- housing_named |&gt;\nrename(Population = B01003_001) |&gt;\nselect(CBSA, NAME, total_units, Population, year) |&gt;\nslice(1)\n\n\n\n\nCode\n# Display as clean data table\ndatatable(\ntop_cbsa,\ncolnames = c(\"CBSA\", \"Name\", \"Total Units\", \"Population\", \"Year\"),\noptions = list(dom = 't', autoWidth = TRUE),\nrownames = FALSE\n) |&gt;\nformatCurrency(columns = c(\"total_units\", \"Population\"), currency = \"\", digits = 0) #commas in numbers \n\n\n\n\n\n\nCode\n# \n\n\n\nIn what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?\n\nHint: There is a Covid-19 data artifact here that may trip you up if you do not look at your answer closely.\nAlbuquerque permitted the most new housing in 2021. However, this year was an anomaly likely due to COVID-19 related slowdowns in the year prior. Other than 2021, the highest number of permits were issued in 2022 when 2,852 new housing units were permitted to be built.\n\n\nCode\n# Make sure PERMITS exists\n\npermits &lt;- get_building_permits()\nlibrary(tidyverse)\nlibrary(DT)\n\n# Filter for Albuquerque, NM (CBSA 10740) and show top 10 years\n\nalbuquerque_top10 &lt;- permits |&gt;\nfilter(CBSA == 10740) |&gt;\narrange(desc(new_housing_units_permitted)) |&gt;\nslice_head(n = 10)\n\n# Display as a clean interactive table\n\ndatatable(\nalbuquerque_top10,\ncolnames = c(\"CBSA\", \"New Units Permitted\", \"Year\"),\noptions = list(dom = 't', autoWidth = TRUE),\nrownames = FALSE\n) |&gt;\nformatCurrency(columns = \"new_housing_units_permitted\", currency = \"\", digits = 0)\n\n\n\n\n\n\nCode\n#\n\n\n\nWhich state (not CBSA) had the highest average individual income in 2015? To answer this question, you will need to first compute the total income per CBSA by multiplying the average household income by the number of households, and then sum total income and total population across all CBSAs in a state. With these numbers, you can answer this question.\n\nThough not technically a state, Washington D.C. had the highest average individual income.\n\n\nCode\nlibrary(dplyr)\nlibrary(stringr)\n\n# 1. Merge the 2015 data sets\nacs_2015 &lt;- INCOME |&gt; \n  filter(year == 2015) |&gt;\n  left_join(HOUSEHOLDS |&gt; filter(year == 2015), by = c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n  left_join(POPULATION |&gt; filter(year == 2015),  by = c(\"GEOID\", \"NAME\", \"year\"))\n\n# 2. Compute total income per CBSA\nacs_2015 &lt;- acs_2015 |&gt;\n  mutate(total_income = household_income * households)\n\n# 3. Extract the principal state abbreviation from the CBSA name\nacs_2015 &lt;- acs_2015 |&gt;\n  mutate(state = str_extract(NAME, \", (.{2})\", group = 1))\n\n# 4. Aggregate to the state level\nstate_summary &lt;- acs_2015 |&gt;\n  group_by(state) |&gt;\n  summarise(\n    total_income = sum(total_income, na.rm = TRUE),\n    total_population = sum(population, na.rm = TRUE)\n  ) |&gt;\n  mutate(avg_individual_income = total_income / total_population)\n\n# 5. Add full state names\nstate_df &lt;- data.frame(\n  abb  = c(state.abb, \"DC\", \"PR\"),\n  name = c(state.name, \"District of Columbia\", \"Puerto Rico\")\n)\n\nstate_summary &lt;- state_summary |&gt;\n  left_join(state_df, by = c(\"state\" = \"abb\")) |&gt;\n  arrange(desc(avg_individual_income))\n\n\n\n\nCode\n#Display data table\n\ndatatable(\n  state_summary |&gt; \n    arrange(desc(avg_individual_income)) |&gt; \n    rename(\n      State = state,\n      `Total Income` = total_income,\n      Population = total_population,\n      `Avg. Individual Income` = avg_individual_income\n    ),\n  options = list(\n    pageLength = 10,    # show 10 rows per page\n    autoWidth = TRUE\n  ),\n  rownames = FALSE\n) |&gt;\n  formatCurrency(\n    columns = c(\"Total Income\", \"Population\", \"Avg. Individual Income\"),\n    currency = \"\", \n    digits = 0\n  )\n\n\n\n\n\n\nCode\n#\n\n\n\nData scientists and business analysts are recorded under NAICS code 5182. What is the last year in which the NYC CBSA had the most data scientists in the country? In recent, the San Francisco CBSA has had the most data scientists.\n\n2015 was the last time that New York had the most data scientists in the country; that year they employed 18,922 data scientists.\n\n\nCode\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(DT)\n\n# Create a new CBSA join key in WAGES\nWAGES &lt;- WAGES |&gt;\n  mutate(\n    CBSA_join = as.double(paste0(str_remove(FIPS, \"C\"), \"0\"))\n  )\n\n# Filter for data scientists (NAICS 5182)\ndata_sci &lt;- WAGES |&gt;\n  filter(INDUSTRY == 5182) |&gt;\n  select(CBSA_join, EMPLOYMENT, YEAR)\n\n# Join to get CBSA names\ndata_sci_named &lt;- data_sci |&gt;\n  inner_join(cbsa_meta |&gt; select(GEOID, NAME), by = c(\"CBSA_join\" = \"GEOID\"))\n\n# Find which CBSA had the most per year\ntop_data_sci_each_year &lt;- data_sci_named |&gt;\n  group_by(YEAR) |&gt;\n  slice_max(order_by = EMPLOYMENT, n = 1, with_ties = FALSE) |&gt;\n  arrange(YEAR)\n\n\n\n\nCode\n# Display as a nice interactive table\ndatatable(\n  top_data_sci_each_year |&gt;\n    rename(\n      Year = YEAR,\n      CBSA = NAME,\n      Employment = EMPLOYMENT\n    ),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  rownames = FALSE\n) |&gt;\n  formatCurrency(columns = \"Employment\", currency = \"\", digits = 0)\n\n\n\n\n\n\nCode\n#\n\n\n\nWhat fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?\n\nIn 2021 the fraction of total wages earned by people in the finance industry peaked at 15.87%\n\n\nCode\n### Q5. What fraction of total wages in the NYC CBSA \n### was earned by people employed in the Finance & Insurance industries (NAICS 52)?\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(scales)\n\n# 1. Compute total wages for all industries in NYC CBSA (C3562)\ntotal_nyc_wages &lt;- WAGES |&gt;\n  filter(FIPS == \"C3562\") |&gt;\n  group_by(YEAR) |&gt;\n  summarise(total_wages = sum(TOTAL_WAGES, na.rm = TRUE))\n\n# 2. Compute total wages for Finance & Insurance (NAICS 52xx)\nfinance_insurance_nyc &lt;- WAGES |&gt;\n  filter(FIPS == \"C3562\", str_starts(as.character(INDUSTRY), \"52\")) |&gt;\n  group_by(YEAR) |&gt;\n  summarise(finance_insurance_wages = sum(TOTAL_WAGES, na.rm = TRUE))\n\n# 3. Combine and compute percent of total\nfinance_insurance_percent &lt;- finance_insurance_nyc |&gt;\n  inner_join(total_nyc_wages, by = \"YEAR\") |&gt;\n  mutate(\n    percent_of_total = (finance_insurance_wages / total_wages) * 100\n  ) |&gt;\n  arrange(desc(percent_of_total)) |&gt;\n  select(YEAR, total_wages, finance_insurance_wages, percent_of_total)\n\n# 4. Print the full table nicely formatted\nprint(\n  finance_insurance_percent,\n  n = nrow(finance_insurance_percent)\n)\n\n# 5. Identify the peak year and percent\npeak_row &lt;- finance_insurance_percent |&gt; slice(1)\npeak_year &lt;- peak_row$YEAR\npeak_share &lt;- percent(peak_row$percent_of_total / 100, accuracy = 0.01)\n\n\n\n\nCode\n# 6. Display concise summary\ncat(\"\\n\")\n\n\nCode\ncat(\"Peak Year:\", peak_year, \"\\n\")\n\n\nPeak Year: 2021 \n\n\nCode\ncat(\"Finance & Insurance Share of Total Wages:\", peak_share, \"\\n\")\n\n\nFinance & Insurance Share of Total Wages: 15.87%"
  },
  {
    "objectID": "mp02.html#data-visualisation",
    "href": "mp02.html#data-visualisation",
    "title": "STA 9750 — Mini-Project 02",
    "section": "Data Visualisation",
    "text": "Data Visualisation"
  },
  {
    "objectID": "mp02.html#data-visualization",
    "href": "mp02.html#data-visualization",
    "title": "STA 9750 — Mini-Project 02",
    "section": "Data Visualization",
    "text": "Data Visualization\n\n\nCode\n#relationship between Monthly Rent and Average Household Income per CBSA in 2009\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(scales)\n\n# 1. Filter and join data for 2009\nrent_income_2009 &lt;- RENT |&gt;\n  filter(year == 2009) |&gt;\n  inner_join(\n    INCOME |&gt;\n      filter(year == 2009),\n    by = \"GEOID\"\n  ) |&gt;\n  rename(\n    monthly_rent = monthly_rent,\n    household_income = household_income\n  ) |&gt;\n  left_join(cbsa_meta |&gt; select(GEOID, NAME), by = \"GEOID\")\n\n# 2. Fit linear model to get R²\nmodel &lt;- lm(monthly_rent ~ household_income, data = rent_income_2009)\nr2_value &lt;- summary(model)$r.squared\n\n# 3. Create scatter plot with R² annotation\nggplot(rent_income_2009, aes(x = household_income, y = monthly_rent)) +\n  geom_point(alpha = 0.6, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkred\", linetype = \"dashed\") +\n  scale_x_continuous(labels = label_comma()) +\n  scale_y_continuous(labels = label_comma()) +\n  annotate(\n    \"text\",\n    x = Inf, y = -Inf,\n    label = paste0(\"R² = \", round(r2_value, 3)),\n    hjust = 1.1, vjust = -1.1,\n    size = 5,\n    color = \"black\"\n  ) +\n  labs(\n    title = \"Monthly Rent vs. Average Household Income per CBSA (2009)\",\n    subtitle = \"Each point represents one CBSA\",\n    x = \"Average Household Income ($)\",\n    y = \"Average Monthly Rent ($)\",\n    caption = \"Source: ACS 1-Year Estimates (2009)\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere is a correlation between rent and household income in 2009. The R-squared value is nearly 0.6, indicating a fairly strong positive correlation between the two. That being said, there is a fair amount deviation from the linear model.\n\n\nCode\n# Health Care & Social Assistance vs. Total Employment (NAICS 62) small multiples plot\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(broom)\n\n# 1) Total employment per CBSA per year\ntotal_emp &lt;- WAGES |&gt;\n  filter(YEAR &gt;= 2013, YEAR &lt;= 2023, YEAR != 2020) |&gt;\n  group_by(FIPS, YEAR) |&gt;\n  summarise(total_employment = sum(as.numeric(EMPLOYMENT), na.rm = TRUE), .groups = \"drop\")\n\n# 2) Health care employment (NAICS 62xx)\nhealth_emp &lt;- WAGES |&gt;\n  filter(YEAR &gt;= 2013, YEAR &lt;= 2023, YEAR != 2020,\n         str_starts(as.character(INDUSTRY), \"62\")) |&gt;\n  group_by(FIPS, YEAR) |&gt;\n  summarise(health_employment = sum(as.numeric(EMPLOYMENT), na.rm = TRUE), .groups = \"drop\")\n\n# 3) Join and compute R² for each year\nemp_combined &lt;- total_emp |&gt;\n  inner_join(health_emp, by = c(\"FIPS\", \"YEAR\"))\n\nr2_by_year &lt;- emp_combined |&gt;\n  group_by(YEAR) |&gt;\n  summarise(\n    r2 = summary(lm(health_employment ~ total_employment))$r.squared,\n    .groups = \"drop\"\n  )\n\n# 4) Plot — clean, professional look\nggplot(emp_combined, aes(x = total_employment, y = health_employment)) +\n  geom_point(alpha = 0.45, size = 0.8, color = \"seagreen4\") +\n  geom_smooth(method = \"lm\", se = FALSE, linetype = \"dashed\", color = \"firebrick3\") +\n  facet_wrap(~ YEAR, ncol = 5, scales = \"fixed\") +\n  # compact axis labels (e.g., 200K, 1M)\n  scale_x_continuous(labels = label_number(scale_cut = cut_short_scale())) +\n  scale_y_continuous(labels = label_number(scale_cut = cut_short_scale())) +\n  labs(\n    title = \"Health Care Employment vs. Total Employment Across CBSAs\",\n    subtitle = \"Small multiples show evolution by year \",\n    x = \"Total Employment \",\n    y = \"Health Care & Social Assistance Employment \",\n    caption = \"Source: BLS QCEW Annual Averages (excludes 2020)\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    strip.text = element_text(face = \"bold\"),\n    axis.text.x = element_text(size = 8),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(color = \"grey90\")\n  ) \n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\n  # Annotate R² per facet\n\n\nThis plot shows the correlation between healthcare employment and total employment and how it has changed over the years. Based on the data, the strong linear correlation has been virtually unchanged; the s-squared fluctuating only about 0.01 over time.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\n\n# Combine and compute household size\nhousehold_size &lt;- HOUSEHOLDS |&gt;\n  select(GEOID, NAME, households, year) |&gt;\n  left_join(\n    POPULATION |&gt; select(GEOID, population, year),\n    by = c(\"GEOID\", \"year\")\n  ) |&gt;\n  mutate(avg_household_size = population / households)\n\n# Label NYC and LA dynamically (regex-safe)\nhousehold_size &lt;- household_size |&gt;\n  mutate(highlight = case_when(\n    str_detect(NAME, regex(\"New York\", ignore_case = TRUE)) ~ \"New York\",\n    str_detect(NAME, regex(\"Los Angeles\", ignore_case = TRUE)) ~ \"Los Angeles\",\n    TRUE ~ \"Other CBSAs\"\n  ))\n\n# Plot\nggplot(household_size, aes(x = year, y = avg_household_size, group = NAME)) +\n  geom_line(aes(color = highlight), alpha = 0.8, linewidth = 0.6) +\n  scale_color_manual(\n    values = c(\n      \"New York\" = \"steelblue\",\n      \"Los Angeles\" = \"darkorange\",\n      \"Other CBSAs\" = \"grey80\"\n    )\n  ) +\n  labs(\n    title = \"Time vs. Average Household Size\",\n    subtitle = \"New York and Los Angeles highlighted\",\n    x = \"Year\",\n    y = \"Average Household Size\",\n    color = \"CBSA\",\n    caption = \"Source: ACS 1-Year Estimates (excluding 2020)\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nThough jumbled, this plot is showing us the average household size and how it has been changing over the years. Each line represents a different CBSA. The blue and orange lines are highlighted to show NYC and LA. The graph is showing a downward trend, particularly in large cities like L.A. and NYC."
  },
  {
    "objectID": "mp02.html#building-indices-of-housing-affordability-and-housing-stock-growth",
    "href": "mp02.html#building-indices-of-housing-affordability-and-housing-stock-growth",
    "title": "STA 9750 — Mini-Project 02",
    "section": "Building Indices of Housing Affordability and Housing Stock Growth",
    "text": "Building Indices of Housing Affordability and Housing Stock Growth\n\nRent Burden Table\n\nRent to income ratio: annual rent/ annual income\nRent burden index: we will use the 2009 as the baseline rent to income ratio and calculate the relative rent burden for the subsequent years using this value.\n2009 represents 100%, anything above will mean that the rent burden has increased, anything below 100% will mean that the burden has decreased relative to 2009.\n\n\n\nCode\n##Task 4\nlibrary(dplyr)\n\n# 1. Merge INCOME and RENT\nrent_burden &lt;- INCOME |&gt;\n  select(GEOID, NAME, year, household_income) |&gt;\n  left_join(\n    RENT |&gt; select(GEOID, year, monthly_rent),\n    by = c(\"GEOID\", \"year\")\n  ) |&gt;\n  mutate(\n    # raw rent burden = annual rent / annual income\n    rent_to_income = (12 * monthly_rent) / household_income\n  ) |&gt;\n  filter(is.finite(rent_to_income))\n\n# 2. Compute the baseline (national average rent burden in 2009)\nbaseline_2009 &lt;- rent_burden |&gt;\n  filter(year == 2009) |&gt;\n  summarise(baseline = mean(rent_to_income, na.rm = TRUE)) |&gt;\n  pull(baseline)\n\n# 3. Create standardized measures\nrent_burden &lt;- rent_burden |&gt;\n  mutate(\n    rent_burden_relative = rent_to_income / baseline_2009,  # multiple of 2009 burden\n    rent_burden_index = rent_burden_relative * 100          # percent of baseline\n  )\n\n\n\n\nCode\nlibrary(DT)\nlibrary(dplyr)\n\n# NYC table\nnyc_rent_burden &lt;- rent_burden |&gt;\n  filter(NAME == \"New York-Newark-Jersey City, NY-NJ-PA Metro Area\") |&gt;\n  arrange(year) |&gt;\n  select(\n    year,\n    rent_to_income,\n    rent_burden_index\n  )\n\ndatatable(\n  nyc_rent_burden,\n  colnames = c(\"Year\", \"Rent-to-Income (%)\", \"Rent Burden Index (%)\"),\n  caption = htmltools::tags$caption(\n    style = '\n      caption-side: top;\n      text-align: center;\n      font-weight: bold;\n      color: black;\n      font-size: 18px;\n    ',\n    \"New York Metro Area — Rent Burden Over a Ten Year Period\"\n  ),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  rownames = FALSE\n) |&gt;\n\n  formatPercentage(columns = \"rent_to_income\", digits = 1) |&gt;\n    formatRound(columns = \"rent_burden_index\", digits = 1)\n\n\n\n\n\n\n\n\nCode\n#Scatterplot\nnyc_plot_data &lt;- rent_burden |&gt;\n  filter(NAME == \"New York-Newark-Jersey City, NY-NJ-PA Metro Area\") |&gt;\n  arrange(year)\n\nnyc_plot_data$year &lt;- as.factor(nyc_plot_data$year)\n\nggplot(nyc_plot_data, aes(x = year, y = rent_burden_index, group = 1)) +\n  geom_point(size = 2, color = \"steelblue\") +\n  geom_line(color = \"steelblue\", linewidth = 1) +\n  labs(\n    title = \"NYC Rent Burden Index Over Time\",\n    subtitle = \"Standardized so 100 = national average rent burden in 2009\",\n    x = \"Year\",\n    y = \"Rent Burden Index (2009 = 100)\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\nThis plot is showing the changes in rent burden in NYC from 2013 to 2023 in reference to 2009. Notice that the rent burden has fluctuated with no real patten over the past decade.\n\n\nCode\n#Comparing metro areas rent burdens\nlibrary(dplyr)\nlibrary(DT)\n\n# Identify latest year\nlatest_year &lt;- max(rent_burden$year)\n\n# Compute rankings\nrent_burden_ranked &lt;- rent_burden |&gt;\n  filter(year == latest_year) |&gt;\n  arrange(desc(rent_burden_index)) |&gt;\n  select(NAME, year, rent_burden_index, rent_to_income)\n\n# Top + bottom 10\nhighest_10 &lt;- rent_burden_ranked |&gt; slice_head(n = 10)\nlowest_10  &lt;- rent_burden_ranked |&gt; slice_tail(n = 10)\n#make highest burden datatable\ndatatable(\n  highest_10,\n  caption = htmltools::tags$caption(\n    style = '\n      caption-side: top;\n      text-align: center;\n      font-weight: 900;\n      color: black;\n      font-size: 20px;\n    ',\n    paste0(\"Top 10 Highest Rent Burden Metros in \", latest_year)\n  ),\n  rownames = FALSE\n) |&gt;\n  formatPercentage(\"rent_to_income\", digits = 1) |&gt;\n  formatRound(\"rent_burden_index\", digits = 1)\n\n\n\n\n\n\nCode\n#Lowest burden\ndatatable(\n  lowest_10,\n  caption = htmltools::tags$caption(\n    style = '\n      caption-side: top;\n      text-align: center;\n      font-weight: 900;\n      color: black;\n      font-size: 20px;\n    ',\n    paste0(\"Top 10 Lowest Rent Burden Metros in \", latest_year)\n  ),\n  rownames = FALSE\n) |&gt;\n  formatPercentage(\"rent_to_income\", digits = 1) |&gt;\n  formatRound(\"rent_burden_index\", digits = 1)\n\n\n\n\n\n\nCode\n#scatterplot\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(scales)\n\n# Pick latest available year\nlatest_year &lt;- max(rent_burden$year, na.rm = TRUE)\n\n# Filter for that year\nrb_latest &lt;- rent_burden |&gt;\n  filter(year == latest_year)\n\n# Identify high/low groups\nn_high &lt;- 10\nn_low  &lt;- 10\n\nhigh_ids &lt;- rb_latest |&gt; slice_max(rent_burden_index, n = n_high) |&gt; pull(GEOID)\nlow_ids  &lt;- rb_latest |&gt; slice_min(rent_burden_index, n = n_low) |&gt; pull(GEOID)\n\n# Classification\nrb_latest &lt;- rb_latest |&gt;\n  mutate(\n    group = case_when(\n      GEOID %in% high_ids ~ \"10 Most Rent Burdened\",\n      GEOID %in% low_ids  ~ \"10 Least Rent Burdened\",\n      TRUE ~ \"Other\"\n    )\n  )\n\n# Scatter plot\nggplot(rb_latest, aes(x = household_income, y = rent_burden_index)) +\n  geom_point(aes(color = group), size = 3, alpha = 0.8) +\n  \n\n  scale_x_continuous(labels = label_comma()) +\n  scale_y_continuous(labels = label_comma()) +\n\n  scale_color_manual(\n    values = c(\n      \"10 Most Rent Burdened\" = \"red\",\n      \"10 Least Rent Burdened\" = \"green4\",\n      \"Other\" = \"gray70\"\n    )\n  ) +\n  \n  labs(\n    title = paste0(\"Highest vs. Lowest Rent Burden Metros (\", latest_year, \")\"),\n    subtitle = \"\",\n    x = \"Average Household Income ($)\",\n    y = \"Rent Burden Index\",\n    color = \"\"\n  ) +\n  \n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", color = \"black\", hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\",\n    legend.box = \"horizontal\",\n    legend.spacing.x = unit(0.5, \"cm\"),\n    legend.title = element_text(size = 12, face = \"bold\"),\n    legend.text = element_text(size = 12)\n  )"
  },
  {
    "objectID": "mp02.html#housing-growth",
    "href": "mp02.html#housing-growth",
    "title": "STA 9750 — Mini-Project 02",
    "section": "Housing Growth",
    "text": "Housing Growth\nWe will now identify the most building friendly metro areas using two methods.\nFirst, we will look at the instantaneous housing growth by measuring how much they build right now given their size. To do this we will measure annual permits per 10,000 people. 2014 is our baseline for the intensity index.\nSecond, we evaluate whether metros are building enough given how fast they are growing. To do this, we measure each metro’s population growth over a fixed 5-year window. Then we relate current annual permits to that 5-year population increase, giving us permits per new resident. We again use the 2014 national mean as a baseline to form the growth index since it is the first 5 year window in our data.\nFinally, we combine these two pieces into a composite measure that reflects both how much a metro builds per capita and how well it keeps up with its own population growth. This composite score helps us identify metros that consistently build at strong levels across both dimensions.\n\nCode\n# task 5 \n#join together the POPULATION and PERMITS tables. Using this data, construct a suitable measure of housing growth: that is, how many new housing units are permitted in a CBSA, relative to both the current number of residents and the overall population growth of that CBSA. Because this metric takes into account growth patterns, it should depend on a fixed lookback-window of 5 years used to estimate population growth.\n\n# Housing Growth\n\nlibrary(dplyr)\nlibrary(DT)\nlibrary(scales)\nlibrary(stringr)\n\n# helper to shorten metro names to first three words\nshort_name3 &lt;- function(x) {\n  sapply(strsplit(x, \" \"), function(y) paste(head(y, 3), collapse = \" \"))\n}\n\n# join POPULATION + PERMITS\npop_housing &lt;- POPULATION %&gt;%\n  select(GEOID, NAME, year, population) %&gt;%\n  left_join(\n    PERMITS %&gt;%\n      rename(\n        GEOID = CBSA,\n        permits = new_housing_units_permitted\n      ),\n    by = c(\"GEOID\", \"year\")\n  )\n\n# compute 5 year population growth within each metro\npop_housing &lt;- pop_housing %&gt;%\n  arrange(GEOID, year) %&gt;%\n  group_by(GEOID) %&gt;%\n  mutate(\n    pop_lag5 = lag(population, 5),\n    pop_growth_5yr = population - pop_lag5\n  ) %&gt;%\n  ungroup()\n\n# instantaneous measure permits per 10k residents and index baseline 2014 mean = 100\npop_housing &lt;- pop_housing %&gt;%\n  mutate(permits_per_10k = (permits / population) * 10000)\n\nbaseline_intensity_2014 &lt;- pop_housing %&gt;%\n  filter(year == 2014) %&gt;%\n  summarise(mean_10k = mean(permits_per_10k, na.rm = TRUE)) %&gt;%\n  pull(mean_10k)\n\npop_housing &lt;- pop_housing %&gt;%\n  mutate(intensity_index = (permits_per_10k / baseline_intensity_2014) * 100)\n\n# rate based measure permits relative to 5 year population growth and index baseline 2014 mean = 100\npop_housing &lt;- pop_housing %&gt;%\n  mutate(permits_per_growth = permits / pop_growth_5yr)\n\nbaseline_growth_2014 &lt;- pop_housing %&gt;%\n  filter(year == 2014) %&gt;%\n  summarise(mean_growth = mean(permits_per_growth, na.rm = TRUE)) %&gt;%\n  pull(mean_growth)\n\npop_housing &lt;- pop_housing %&gt;%\n  mutate(growth_index = (permits_per_growth / baseline_growth_2014) * 100)\n\n# composite score no weights simple average of the two indices\npop_housing &lt;- pop_housing %&gt;%\n  mutate(composite_score = (intensity_index + growth_index) / 2)\n\n# choose most recent year available for tables\nlatest_year &lt;- max(pop_housing$year, na.rm = TRUE)\n\n# build instantaneous table top 10 and bottom 10 by permits per 10k in latest year\ninst_tbl &lt;- pop_housing %&gt;%\n  filter(year == latest_year, is.finite(permits_per_10k)) %&gt;%\n  mutate(\n    Category = if_else(intensity_index &gt;= 100, \"HIGH\", \"LOW\"),\n    `Metro Area` = short_name3(NAME)\n  ) %&gt;%\n  select(\n    Category,\n    `Metro Area`,\n    Population = population,\n    `Annual Permits` = permits,\n    `Permits per 10k` = permits_per_10k,\n    `Intensity Index` = intensity_index\n  )\n\ninst_high &lt;- inst_tbl %&gt;%\n  filter(Category == \"HIGH\") %&gt;%\n  arrange(desc(`Permits per 10k`)) %&gt;%\n  slice_head(n = 10)\n\ninst_low &lt;- inst_tbl %&gt;%\n  filter(Category == \"LOW\") %&gt;%\n  arrange(`Permits per 10k`) %&gt;%\n  slice_head(n = 10)\n\ninst_final &lt;- bind_rows(inst_high, inst_low)\n\ndatatable(\n  inst_final %&gt;%\n    mutate(\n      Population = comma(Population),\n      `Annual Permits` = comma(`Annual Permits`),\n      `Permits per 10k` = round(`Permits per 10k`, 2),\n      `Intensity Index` = round(`Intensity Index`, 1)\n    ),\n  rownames = FALSE,\n  options = list(\n    dom = 't',\n    pageLength = 20,\n    autoWidth = TRUE\n  ),\n   caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; margin-bottom: 10px;',\n    htmltools::tags$div(\n      style = 'font-size: 16pt; font-weight: bold;',\n      paste0(\"Permits Per Capita In \", latest_year)\n    )\n  )\n)\n\n\n\n\nCode\n# build growth adjusted table top 10 and bottom 10 by permits per growth in latest year\ngrowth_tbl &lt;- pop_housing %&gt;%\n  filter(year == latest_year, is.finite(permits_per_growth)) %&gt;%\n  mutate(\n    Category = if_else(growth_index &gt;= 100, \"HIGH\", \"LOW\"),\n    `Metro Area` = short_name3(NAME)\n  ) %&gt;%\n  select(\n    Category,\n    `Metro Area`,\n    `5 Year Population Growth` = pop_growth_5yr,\n    `Annual Permits` = permits,\n    `Permits per Growth` = permits_per_growth,\n    `Growth Index` = growth_index\n  )\n\ngrowth_high &lt;- growth_tbl %&gt;%\n  filter(Category == \"HIGH\") %&gt;%\n  arrange(desc(`Permits per Growth`)) %&gt;%\n  slice_head(n = 10)\n\ngrowth_low &lt;- growth_tbl %&gt;%\n  filter(Category == \"LOW\") %&gt;%\n  arrange(`Permits per Growth`) %&gt;%\n  slice_head(n = 10)\n\ngrowth_final &lt;- bind_rows(growth_high, growth_low)\n\ndatatable(\n  growth_final %&gt;%\n    mutate(\n      `5 Year Population Growth` = comma(`5 Year Population Growth`),\n      `Annual Permits` = comma(`Annual Permits`),\n      `Permits per Growth` = round(`Permits per Growth`, 3),\n      `Growth Index` = round(`Growth Index`, 1)\n    ),\n  rownames = FALSE,\n  options = list(\n    dom = 't',\n    pageLength = 20,\n    autoWidth = TRUE\n  ),\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; margin-bottom: 10px;',\n    htmltools::tags$div(\n      style = 'font-size: 16pt; font-weight: bold;',\n      paste0(\"Growth Adjusted Housing Supply In \", latest_year)\n    )\n  )\n)\n\n\n\n\nCode\n# build composite table top 10 and bottom 10 by composite score\ncomp_tbl &lt;- pop_housing %&gt;%\n  filter(year == latest_year, is.finite(composite_score)) %&gt;%\n  mutate(\n    `Metro Area` = short_name3(NAME)\n  ) %&gt;%\n  select(\n    `Metro Area`,\n    Population = population,\n    `Annual Permits` = permits,\n    `Permits per 10k` = permits_per_10k,\n    `Intensity Index` = intensity_index,\n    `Growth Index` = growth_index,\n    `Composite Score` = composite_score\n  )\n\ncomp_high &lt;- comp_tbl %&gt;%\n  arrange(desc(`Composite Score`)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(Category = \"HIGHEST COMPOSITE\")\n\ncomp_low &lt;- comp_tbl %&gt;%\n  arrange(`Composite Score`) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(Category = \"LOWEST COMPOSITE\")\n\ncomp_final &lt;- bind_rows(comp_high, comp_low) %&gt;%\n  relocate(Category, .before = 1)\n\ndatatable(\n  comp_final %&gt;%\n    mutate(\n      Population = comma(Population),\n      `Annual Permits` = comma(`Annual Permits`),\n      `Permits per 10k` = round(`Permits per 10k`, 2),\n      `Intensity Index` = round(`Intensity Index`, 1),\n      `Growth Index` = round(`Growth Index`, 1),\n      `Composite Score` = round(`Composite Score`, 1)\n    ),\n  rownames = FALSE,\n  options = list(\n    dom = 't',\n    pageLength = 20,\n    autoWidth = TRUE\n  ),\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; margin-bottom: 10px;',\n    htmltools::tags$div(\n      style = 'font-size: 16pt; font-weight: bold;',\n      paste0(\"Composite Housing Growth Score In \", latest_year)\n    )\n  )\n)"
  },
  {
    "objectID": "mp02.html#visualizations-for-identifying-yimby-cities",
    "href": "mp02.html#visualizations-for-identifying-yimby-cities",
    "title": "STA 9750 — Mini-Project 02",
    "section": "Visualizations For Identifying YIMBY Cities",
    "text": "Visualizations For Identifying YIMBY Cities\n\n\nCode\n# Task 6: Visualizations\n# We relate rent burden change, population growth, and housing growth\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(tidyr)\n\n# Compute rent burden change\nrent_change &lt;- rent_burden %&gt;%\n  filter(year %in% c(2009, 2023)) %&gt;%\n  select(GEOID, year, rent_burden_index) %&gt;%\n  pivot_wider(\n    names_from = year,\n    values_from = rent_burden_index,\n    names_prefix = \"rb_\"\n  ) %&gt;%\n  mutate(rent_change = rb_2023 - rb_2009)\n\n# Compute population change 2009–2023\npop_change &lt;- POPULATION %&gt;%\n  filter(year %in% c(2009, 2023)) %&gt;%\n  select(GEOID, population, year) %&gt;%\n  pivot_wider(\n    names_from = year,\n    values_from = population,\n    names_prefix = \"pop_\"\n  ) %&gt;%\n  mutate(pop_change = pop_2023 - pop_2009)\n\n# Build task 6 dataset\ntask6 &lt;- pop_housing %&gt;%\n  filter(year == 2023) %&gt;%\n  select(GEOID, permits_per_10k, intensity_index, pop_growth_5yr, growth_index) %&gt;%\n  left_join(POPULATION %&gt;% select(GEOID, NAME), by = \"GEOID\") %&gt;%\n  left_join(rent_change, by = \"GEOID\") %&gt;%\n  left_join(pop_change, by = \"GEOID\") %&gt;%\n  mutate(\n    yimby_flag = if_else(\n      rb_2009 &gt; median(rb_2009, na.rm = TRUE) &\n      rent_change &lt; 0 &\n      pop_change &gt; 0 &\n      intensity_index &gt; median(intensity_index, na.rm = TRUE) &\n      growth_index &gt; median(growth_index, na.rm = TRUE),\n      \"YIMBY Candidate\", \"Other\"\n    )\n  )\n\n# Color palette\ncbsa_palette &lt;- c(\n  \"YIMBY Candidate\" = \"#1b9e77\",\n  \"Other\" = \"#7570b3\"\n)\n\n\nThe following plot examines how changes in rent burden from 2009 to 2023 relate to per-capita housing production in 2023, measured as permits per 10,000 residents. We created this plot to see which metros, if any, build more housing on a per-resident basis and experience smaller increases in rent burden over time.\nThe “YIMBY Candidate” metros are those that experienced above-median housing production (as measured by the intensity index) and also saw rent-burden improvement.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(tidyr)\n\n# Compute rent burden change 2009 → 2023 \nrent_change &lt;- rent_burden %&gt;%\n  filter(year %in% c(2009, 2023)) %&gt;%\n  select(GEOID, year, rent_burden_index) %&gt;%\n  pivot_wider(\n    names_from = year,\n    values_from = rent_burden_index,\n    names_prefix = \"rb_\"\n  ) %&gt;%\n  mutate(rent_change = rb_2023 - rb_2009)\n\n# compute population change 2009 → 2023 \npop_change &lt;- POPULATION %&gt;%\n  filter(year %in% c(2009, 2023)) %&gt;%\n  select(GEOID, population, year) %&gt;%\n  pivot_wider(\n    names_from = year,\n    values_from = population,\n    names_prefix = \"pop_\"\n  ) %&gt;%\n  mutate(pop_change = pop_2023 - pop_2009)\n\n# data\ntask6 &lt;- pop_housing %&gt;%\n  filter(year == 2023) %&gt;%\n  select(GEOID, permits_per_10k, intensity_index, pop_growth_5yr, growth_index) %&gt;%\n  left_join(POPULATION %&gt;% select(GEOID, NAME), by = \"GEOID\") %&gt;%\n  left_join(rent_change, by = \"GEOID\") %&gt;%\n  left_join(pop_change, by = \"GEOID\") %&gt;%\n  mutate(\n    yimby_flag = if_else(\n      rb_2009 &gt; median(rb_2009, na.rm = TRUE) &\n      rent_change &lt; 0 &\n      pop_change &gt; 0 &\n      intensity_index &gt; median(intensity_index, na.rm = TRUE) &\n      growth_index &gt; median(growth_index, na.rm = TRUE),\n      \"YIMBY Candidate\",\n      \"Other\"\n    )\n  )\n\n# Remove rows with missing values for the plotted vars \nplot_data &lt;- task6 %&gt;%\n  filter(\n    is.finite(rent_change),\n    is.finite(permits_per_10k)\n  )\n\n# plot\nggplot(plot_data, aes(x = rent_change, y = permits_per_10k, color = yimby_flag)) +\n  geom_point(size = 3, alpha = 0.85) +\n  scale_color_manual(values = c(\"YIMBY Candidate\" = \"#1b9e77\", \"Other\" = \"#7570b3\")) +\n  labs(\n    title = \"Rent Burden  vs  Housing Production\",\n    x = \"Change in Rent Burden (Index Points)\",\n    y = \"Permits per 10,000 Residents\",\n    color = \"\"\n  ) +\ntheme_minimal(base_size = 15) +\ntheme(\n    legend.position = \"bottom\",\n    plot.title = element_text(size = 20, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 14, hjust = 0.5)\n)\n\n\n\n\n\n\n\n\n\nThe following two small-multiple plots take a closer look at the six metros that emerged as the strongest YIMBY candidates: Rocky Mount NC, Harrisonburg VA, Yuma AZ, Winchester VA WV, Gulfport Biloxi MS, and Laredo TX. In the first plot, we can see that all six metros experienced a decline in rent burden over time, which is exactly what we would hope to see in places that are building enough housing to keep pressure off prices.\nThe second plot shows how much these areas have been building relative to their size, and for most of them the story is consistent. Rocky Mount, Harrisonburg, Yuma, Winchester, and Laredo all exhibit strong per-capita housing production, reinforcing why they score highly as YIMBY metros.\nGulfport Biloxi is the one partial outlier: it did reduce rent burden, but its permitting activity does not increase nearly as much as the others. This could mean its affordability gains are driven more by slower demand or population changes than by sustained housing growth.\n\n\nCode\n#Plot 2a\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\n\n# Ensure one row per CBSA\ntask6_unique &lt;- task6 %&gt;% distinct(GEOID, .keep_all = TRUE)\n\n# 2. Filter metros meeting BOTH criteria\n# - rent burden decreased more than typical\n# - housing production per capita higher than typical\n\nyimby_filtered &lt;- task6_unique %&gt;%\n  filter(\n    is.finite(rent_change),\n    is.finite(permits_per_10k)\n  ) %&gt;%\n  filter(\n    rent_change &lt; median(rent_change, na.rm = TRUE),\n    permits_per_10k &gt; median(permits_per_10k, na.rm = TRUE)\n  )\n\n# 3. Pick the TOP 6 best performers:\n#    - most rent burden decrease (more negative is better)\n#    - more permits per 10k secondarily\n\ntop6 &lt;- yimby_filtered %&gt;%\n  arrange(rent_change, desc(permits_per_10k)) %&gt;%\n  slice_head(n = 6) %&gt;%\n  pull(NAME)\n\n# 4. Prepare rent burden time series for these 6 metros\n\nshort_name &lt;- function(x) {\n  sapply(strsplit(x, \" \"), function(y) paste(head(y, 2), collapse = \" \"))\n}\n\nyimby_rent_trends &lt;- rent_burden %&gt;%\n  filter(NAME %in% top6) %&gt;%\n  mutate(short = short_name(NAME))\n\n# 5. Plot 2A — Rent Burden Trends\n\nggplot(yimby_rent_trends, aes(x = year, y = rent_burden_index)) +\n  geom_line(color = \"#1b9e77\", linewidth = 1) +\n  geom_point(color = \"#1b9e77\", size = 2) +\n  facet_wrap(~ short, ncol = 3, scales = \"free_y\") +\n  scale_y_continuous(labels = label_comma()) +\n  labs(\n    title = \"Rent Burden Trends for Top 6 YIMBY Metros\",\n    subtitle = \"Metros that both reduced rent burden and built more housing per capita\",\n    x = \"Year\",\n    y = \"Rent Burden Index (2009 = 100)\"\n  ) +\n  theme_minimal(base_size = 16) +\n  theme(\n    strip.text = element_text(size = 9, face = \"bold\"),\n    plot.title = element_text(face = \"bold\", size = 20, hjust = 0.5),\n    plot.subtitle = element_text(size = 14, hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 10),\n    axis.text.y = element_text(size = 10),\n    panel.spacing = unit(1.2, \"lines\")\n  )\n\n\n\n\n\n\n\n\n\nCode\n#2b plot\nlibrary(stringr)\n\n# (Optional: If you prefer YOUR manually chosen 6)\n# top6 &lt;- c(\n#   \"Rocky Mount, NC Metro Area\",\n#   \"Harrisonburg, VA Metro Area\",\n#   \"Yuma, AZ Metro Area\",\n#   \"Winchester, VA-WV Metro Area\",\n#   \"Gulfport-Biloxi, MS Metro Area\",\n#   \"Laredo, TX Metro Area\"\n# )\n\nyimby_housing_trends &lt;- pop_housing %&gt;%\n  filter(NAME %in% top6) %&gt;%\n  mutate(short = short_name(NAME))\n\nggplot(yimby_housing_trends, aes(x = year, y = permits_per_10k)) +\n  geom_line(color = \"#1b9e77\", linewidth = 1) +\n  geom_point(color = \"#1b9e77\", size = 2) +\n  facet_wrap(~ short, ncol = 3, scales = \"free_y\") +\n  scale_y_continuous(labels = label_comma()) +\n  labs(\n    title = \"Housing Production Over Time\",\n    subtitle = \"Top 6 YIMBY Candidate Metros Identified in Scatterplot\",\n    x = \"Year\",\n    y = \"Permits per 10,000 Residents\"\n  ) +\n  theme_minimal(base_size = 16) +\n  theme(\n    strip.text = element_text(size = 9, face = \"bold\"),\n    plot.title = element_text(face = \"bold\", size = 20, hjust = 0.5),\n    plot.subtitle = element_text(size = 14, hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 10),\n    panel.spacing = unit(1.2, \"lines\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n#data for Policy breif\n# Find start and end values for Rocky Mount\nrb_rm &lt;- rent_burden %&gt;%\n  filter(NAME == \"Rocky Mount, NC Metro Area\") %&gt;%\n  arrange(year)\n\nrb_rm_start &lt;- rb_rm$rent_burden_index[1]\nrb_rm_end   &lt;- rb_rm$rent_burden_index[nrow(rb_rm)]\n\nrent_burden_pct_change &lt;- (rb_rm_end - rb_rm_start) / rb_rm_start * 100\nrent_burden_pct_change\n\n\n[1] -21.1345\n\n\nCode\n# Compute Rocky Mount housing production % change using permits_per_10k\n\nhg_rm &lt;- pop_housing %&gt;%\n  filter(NAME == \"Rocky Mount, NC Metro Area\") %&gt;%\n  arrange(year)\n\nhg_rm_start &lt;- hg_rm$permits_per_10k[1]\nhg_rm_end   &lt;- hg_rm$permits_per_10k[nrow(hg_rm)]\n\nhousing_growth_pct_change &lt;- (hg_rm_end - hg_rm_start) / hg_rm_start * 100\nhousing_growth_pct_change\n\n\n[1] 128.4586"
  },
  {
    "objectID": "mp02.html#backyards-for-all-bill",
    "href": "mp02.html#backyards-for-all-bill",
    "title": "STA 9750 — Mini-Project 02",
    "section": "Backyards For All Bill",
    "text": "Backyards For All Bill\n\nWhy All Americans Deserve Affordable Housing\nA federal nudge for local YIMBY wins America’s housing shortage continues to push rents upward and limit economic mobility. This proposal establishes a competitive federal program that rewards cities for modernizing zoning, approving homes more efficiently, and reducing rent burdens. The goal is simple: accelerate homebuilding in the places where Americans already live, work, and want to stay. Proposed bill sponsors Primary sponsor: A representative from Rocky Mount, NC Metro Area — one of the clearest YIMBY successes in our analysis. Rocky Mount permitted housing at strong per-capita levels and saw a meaningful decline in rent burden over time. Co-sponsor: A representative from New York City, NY Metro Area — a high-rent, slower-to-build region where the affordability crisis is sharpest and federal incentives would have the biggest impact. Why these hometowns Rocky Mount, NC: Rocky Mount provides a strong success story: its permitting levels rose relative to population and its rent burden fell. That’s exactly the pattern federal policymakers want to scale. A sponsor from this district can credibly argue that pro-housing reforms work — and that federal grants for infrastructure upgrades, digital permitting, and missing-middle zoning could help more cities follow the same trajectory. New York City, NY: NYC’s severe rent burden and decades of underbuilding highlight why a federal partner is needed. This bill gives NYC tools to update zoning around transit, speed approvals for infill and conversions, expand ADUs, and modernize permitting systems. The city stands to benefit immensely from incentive funding tied to measurable progress. Coalition building: occupations to mobilize Nurses and other healthcare workers (SEIU, National Nurses United). Both Rocky Mount and NYC have substantial healthcare workforces. In cities that permit more housing, rent burdens fall, leaving nurses with more disposable income and reducing turnover for hospitals and clinics. This stabilizes staffing — a top concern for these unions. Teachers and school staff (AFT, NEA). Educators make up a large, locally rooted voting bloc in both metros. Lower rent burden helps teachers stay in their districts instead of commuting long distances or leaving the profession. More housing near schools also stabilizes neighborhood enrollment. What the program funds Performance-based grants for cities that legalize more homes near jobs and transit, digitize their permitting processes, and demonstrate progress in lowering rent burden. Technical assistance including model zoning codes, pre-approved residential plans, and support for ADUs and missing-middle housing. Affordability protections such as mobility assistance, shallow rent supports, and right-to-counsel pilots during transitions. How we measure success (plain-English metrics) Rent-burden index: We compare typical rent to typical household income and scale everything so 2009 = 100. When a city’s index falls below 100 over time, residents are spending a smaller share of their income on rent — a clear sign of improved affordability. Housing-growth metrics: Permits per 10,000 residents: A per-capita measure of how much a city is building today, allowing for fair comparison across metros. Growth-adjusted index: Permits relative to each city’s five-year population growth, answering whether a city is building enough to keep up with demand. Together, these measures reward cities that both build more housing and reduce rent pressure."
  },
  {
    "objectID": "mp02.html#backyards-for-all-bill-why-all-americans-deserve-affordable-housing",
    "href": "mp02.html#backyards-for-all-bill-why-all-americans-deserve-affordable-housing",
    "title": "STA 9750 — Mini-Project 02",
    "section": "Backyards For All Bill: Why All Americans Deserve Affordable Housing",
    "text": "Backyards For All Bill: Why All Americans Deserve Affordable Housing\nAmerica’s housing shortage continues to push rents upward in many major metropolitan cities, New York being one of the most dire examples, with only around 1% of housing units are vacant. As a result, many working class people are either getting pushed out of their cities, or suffering a loss in standard of living year over year. With general inflation running rampant in recent years, it is imperative that a change be made to bring releif to everyday Americans.\nThis proposal establishes a competitive federal program that rewards cities for modernizing zoning, approving homes more efficiently, and reducing rent burdens. The goal is simple: accelerate homebuilding in the places where Americans already live, work, and want to stay.\nPrimary sponsor:\nA congressional representative from Rocky Mount, NC Metro Area — one of the clearest YIMBY successes in our analysis. Rocky Mount permitted housing at strong per-capita levels and saw a meaningful decline in rent burden over time. Rocky Mount is an ideal sponsor becasue they were able to implement a 28% increase in housing growth which led to a 20% decrease in rent burden for their population.\nCo-sponsor:\nA congressional representative from New York City, NY Metro Area — a high-rent, slower-to-build region where the affordability crisis is sharpest and federal incentives would have the biggest impact.\nSupport groups:\nMobilize Nurses and other healthcare workers (SEIU, National Nurses United).\nBoth Rocky Mount and NYC have substantial healthcare workforces. In cities that permit more housing, rent burdens fall, leaving nurses with more disposable income and reducing turnover for hospitals and clinics. This stabilizes nurse staffing, which is a top concern for these unions.\nTeachers and school staff (AFT, NEA). Educators make up a large, locally rooted voting bloc in both metros. Lower rent burden helps teachers stay in their districts instead of commuting long distances or leaving the profession. Currently, 33% of teachers in NYC live outside of the city due to high rent prices (Independent Budget Office 2019).\nWhat the program funds:\n\nPerformance-based grants for cities that legalize more homes near jobs and transit, digitize their permitting processes, and demonstrate progress in lowering rent burden.\nTechnical assistance including model zoning codes and pre-approved residential plans.\n\nHow we measure success:\n\nRent-burden index: We compare typical rent to typical household income and scale everything so 2009 = 100. When a city’s index falls below 100 over time, residents are spending a smaller share of their income on rent, this is a clear sign of improved affordability.\nHousing-growth metrics: Permits per 10,000 residents: A per-capita measure of how much a city is building today, allowing for fair comparison across metros.\nGrowth-adjusted index: Permits relative to each city’s five-year population growth, answering whether a city is building enough to keep up with demand.\n\nWhy NewYorkers Will all Benefit:\nNew York is known as “the city that never sleeps” however, anyone who lives there will tell you that this saying hardly hold true today. Most shops, restaurants and cafes are closed by 9 or 10pm even on weekends. What caused this? For most of its history, New York was an industrial city. Factory workers kept long, irregular hours and the businesses around them stayed open late to match their schedules. After that era ended, the city became a hub for artists, who also tended to live and work on nontraditional hours, keeping New York’s late night culture alive.\nToday, Manhattan has undergone a wave of corporate consolidation. Large companies have bought up much of the real estate, and as prices rose, artists and other creative workers were pushed to the outer boroughs while corporate employees with more conventional schedules replaced them. Neighborhoods in Brooklyn and Queens still hold onto pieces of the old nightlife energy, but in Manhattan it has mostly faded.\nMaking housing more affordable in Manhattan by building more homes would help reverse this shift. If younger people and artists could actually afford to live there again, the city could begin to regain some of the vibrancy that once defined it. This would not only improve the quality of life for Gen Z and Millennials, but it would also boost the broader city economy. The nightlife sector alone contributes about 35 billion dollars to New York City’s economy, and restoring that culture would help strengthen the city as a whole (Mayor’s Office of Media and Entertainment, 2019).\nTogether, these measures reward cities that both build more housing and reduce rent pressure.\nCitations\nMayor’s Office of Media and Entertainment. (2019). NYC Nightlife Economic Impact Report. Retrieved from https://www.nyc.gov/assets/mome/pdf/NYC_Nightlife_Economic_Impact_Report_2019_digital.pdf\nNew York City Independent Budget Office. (2019). Where Do NYC’s Teachers and Principals Live Compared With Where They Work? Retrieved from https://www.ibo.nyc.ny.us/iboreports/printnycbtn19.pdf"
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "STA 9750 — Mini-Project 02",
    "section": "",
    "text": "Housing affordability has become a huge challenge facing cities across the country, the supply of housing just cannot keep up with the demand. A lot of the debate comes down to two clashing philosophies: YIMBY (“Yes In My Backyard”), which argues that cities should make it easier to build housing, and NIMBY (“Not In My Backyard”), which pushes back against new development out of fear that it will change neighborhood character, lead to gentrification, or hurt property values. It’s a controversial topic because both sides claim to be protecting their communities.\nIn this project, we will dig into Census, ACS and BLS data to see which metro areas are embracing the YIMBY approach and which ones are effectively blocking growth. We will build rent burden and housing growth metrics, track trends across hundreds of CBSAs, and identify a handful of metros that stand out as YIMBY success stories.\n\n\nCode\ntidycensus::census_api_key(\"aa14982ecd4bb502eb657c121f3d02e9b1c45cee\", install = FALSE)\n\ngetwd()\nlist.files(\"data\", recursive = TRUE)\n\n\nif(!dir.exists(file.path(\"data\", \"mp02\"))){\n    dir.create(file.path(\"data\", \"mp02\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nlibrary &lt;- function(pkg){\n    ## Mask base::library() to automatically install packages if needed\n    ## Masking is important here so downlit picks up packages and links\n    ## to documentation\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE, quietly=TRUE))\n}\n\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(readxl)\nlibrary(tidycensus)\n\nget_acs_all_years &lt;- function(variable, geography=\"cbsa\",\n                              start_year=2009, end_year=2023){\n    fname &lt;- glue(\"{variable}_{geography}_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \nif(!file.exists(fname)){\n        YEARS &lt;- seq(start_year, end_year)\n        YEARS &lt;- YEARS[YEARS != 2020] # Drop 2020 - No survey (covid)\n        \n        ALL_DATA &lt;- map(YEARS, function(yy){\n            tidycensus::get_acs(geography, variable, year=yy, survey=\"acs1\") |&gt;\n                mutate(year=yy) |&gt;\n                select(-moe, -variable) |&gt;\n                rename(!!variable := estimate)\n        }) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\n# Household income (12 month)\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;\n    rename(household_income = B19013_001)\n\n# Monthly rent\nRENT &lt;- get_acs_all_years(\"B25064_001\") |&gt;\n    rename(monthly_rent = B25064_001)\n\n# Total population\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;\n    rename(population = B01003_001)\n\n# Total number of households\nHOUSEHOLDS &lt;- get_acs_all_years(\"B11001_001\") |&gt;\n    rename(households = B11001_001)\nget_building_permits &lt;- function(start_year = 2009, end_year = 2023){\n    fname &lt;- glue(\"housing_units_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        HISTORICAL_YEARS &lt;- seq(start_year, 2018)\n        \n        HISTORICAL_DATA &lt;- map(HISTORICAL_YEARS, function(yy){\n            historical_url &lt;- glue(\"https://www.census.gov/construction/bps/txt/tb3u{yy}.txt\")\n                \n            LINES &lt;- readLines(historical_url)[-c(1:11)]\n\n            CBSA_LINES &lt;- str_detect(LINES, \"^[[:digit:]]\")\n            CBSA &lt;- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))\n\n            PERMIT_LINES &lt;- str_detect(str_sub(LINES, 48, 53), \"[[:digit:]]\")\n            PERMITS &lt;- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))\n            \n            data_frame(CBSA = CBSA,\n                       new_housing_units_permitted = PERMITS, \n                       year = yy)\n        }) |&gt; bind_rows()\n        \n        CURRENT_YEARS &lt;- seq(2019, end_year)\n        \n        CURRENT_DATA &lt;- map(CURRENT_YEARS, function(yy){\n            current_url &lt;- glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls\")\n            \n            temp &lt;- tempfile()\n            \n            download.file(current_url, destfile = temp, mode=\"wb\")\n            \n            fallback &lt;- function(.f1, .f2){\n                function(...){\n                    tryCatch(.f1(...), \n                             error=function(e) .f2(...))\n                }\n            }\n            \n            reader &lt;- fallback(read_xlsx, read_xls)\n            \n            reader(temp, skip=5) |&gt;\n                na.omit() |&gt;\n                select(CBSA, Total) |&gt;\n                mutate(year = yy) |&gt;\n                rename(new_housing_units_permitted = Total)\n        }) |&gt; bind_rows()\n        \n        ALL_DATA &lt;- rbind(HISTORICAL_DATA, CURRENT_DATA)\n        \n        write_csv(ALL_DATA, fname)\n        \n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nPERMITS &lt;- get_building_permits()\n\nlibrary(httr2)\nlibrary(rvest)\nget_bls_industry_codes &lt;- function(){\n    fname &lt;- file.path(\"data\", \"mp02\", \"bls_industry_codes.csv\")\n    library(dplyr)\n    library(tidyr)\n    library(readr)\n    \n    if(!file.exists(fname)){\n        \n        resp &lt;- request(\"https://www.bls.gov\") |&gt; \n            req_url_path(\"cew\", \"classifications\", \"industry\", \"industry-titles.htm\") |&gt;\n            req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n            req_error(is_error = \\(resp) FALSE) |&gt;\n            req_perform()\n        \n        resp_check_status(resp)\n        \n        naics_table &lt;- resp_body_html(resp) |&gt;\n            html_element(\"#naics_titles\") |&gt; \n            html_table() |&gt;\n            mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), \"NAICS\"))) |&gt;\n            select(-`Industry Title`) |&gt;\n            mutate(depth = if_else(nchar(Code) &lt;= 5, nchar(Code) - 1, NA)) |&gt;\n            filter(!is.na(depth))\n        \n        # These were looked up manually on bls.gov after finding \n        # they were presented as ranges. Since there are only three\n        # it was easier to manually handle than to special-case everything else\n        naics_missing &lt;- tibble::tribble(\n            ~Code, ~title, ~depth, \n            \"31\", \"Manufacturing\", 1,\n            \"32\", \"Manufacturing\", 1,\n            \"33\", \"Manufacturing\", 1,\n            \"44\", \"Retail\", 1, \n            \"45\", \"Retail\", 1,\n            \"48\", \"Transportation and Warehousing\", 1, \n            \"49\", \"Transportation and Warehousing\", 1\n        )\n        \n        naics_table &lt;- bind_rows(naics_table, naics_missing)\n        \n        naics_table &lt;- naics_table |&gt; \n            filter(depth == 4) |&gt; \n            rename(level4_title=title) |&gt; \n            mutate(level1_code = str_sub(Code, end=2), \n                   level2_code = str_sub(Code, end=3), \n                   level3_code = str_sub(Code, end=4)) |&gt;\n            left_join(naics_table, join_by(level1_code == Code)) |&gt;\n            rename(level1_title=title) |&gt;\n            left_join(naics_table, join_by(level2_code == Code)) |&gt;\n            rename(level2_title=title) |&gt;\n            left_join(naics_table, join_by(level3_code == Code)) |&gt;\n            rename(level3_title=title) |&gt;\n            select(-starts_with(\"depth\")) |&gt;\n            rename(level4_code = Code) |&gt;\n            select(level1_title, level2_title, level3_title, level4_title, \n                   level1_code,  level2_code,  level3_code,  level4_code) |&gt;\n            drop_na() |&gt;\n            mutate(across(contains(\"code\"), as.integer))\n        \n        write_csv(naics_table, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nINDUSTRY_CODES &lt;- get_bls_industry_codes()\nlibrary(httr2)\nlibrary(rvest)\nget_bls_qcew_annual_averages &lt;- function(start_year=2009, end_year=2023){\n    fname &lt;- glue(\"bls_qcew_{start_year}_{end_year}.csv.gz\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    YEARS &lt;- seq(start_year, end_year)\n    YEARS &lt;- YEARS[YEARS != 2020] # Drop Covid year to match ACS\n    \n    if(!file.exists(fname)){\n        ALL_DATA &lt;- map(YEARS, .progress=TRUE, possibly(function(yy){\n            fname_inner &lt;- file.path(\"data\", \"mp02\", glue(\"{yy}_qcew_annual_singlefile.zip\"))\n            \n            if(!file.exists(fname_inner)){\n                request(\"https://www.bls.gov\") |&gt; \n                    req_url_path(\"cew\", \"data\", \"files\", yy, \"csv\",\n                                 glue(\"{yy}_annual_singlefile.zip\")) |&gt;\n                    req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n                    req_retry(max_tries=5) |&gt;\n                    req_perform(fname_inner)\n            }\n            \n            if(file.info(fname_inner)$size &lt; 755e5){\n                warning(sQuote(fname_inner), \"appears corrupted. Please delete and retry this step.\")\n            }\n            \n            read_csv(fname_inner, \n                     show_col_types=FALSE) |&gt; \n                mutate(YEAR = yy) |&gt;\n                select(area_fips, \n                       industry_code, \n                       annual_avg_emplvl, \n                       total_annual_wages, \n                       YEAR) |&gt;\n                filter(nchar(industry_code) &lt;= 5, \n                       str_starts(area_fips, \"C\")) |&gt;\n                filter(str_detect(industry_code, \"-\", negate=TRUE)) |&gt;\n                mutate(FIPS = area_fips, \n                       INDUSTRY = as.integer(industry_code), \n                       EMPLOYMENT = as.integer(annual_avg_emplvl), \n                       TOTAL_WAGES = total_annual_wages) |&gt;\n                select(-area_fips, \n                       -industry_code, \n                       -annual_avg_emplvl, \n                       -total_annual_wages) |&gt;\n                # 10 is a special value: \"all industries\" , so omit\n                filter(INDUSTRY != 10) |&gt; \n                mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)\n        })) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    ALL_DATA &lt;- read_csv(fname, show_col_types=FALSE)\n    \n    ALL_DATA_YEARS &lt;- unique(ALL_DATA$YEAR)\n    \n    YEARS_DIFF &lt;- setdiff(YEARS, ALL_DATA_YEARS)\n    \n    if(length(YEARS_DIFF) &gt; 0){\n        stop(\"Download failed for the following years: \", YEARS_DIFF, \n             \". Please delete intermediate files and try again.\")\n    }\n    \n    ALL_DATA\n}\n\nWAGES &lt;- get_bls_qcew_annual_averages()"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini Project 03",
    "section": "",
    "text": "New York City’s street trees play a crucial role in shaping the city’s environment and public spaces, but they also face challenges ranging from invasive species to uneven maintenance across neighborhoods. In this project, we will analyze the NYC TreeMap data, which catalogs tree species, health, and location across all council districts, to uncover patterns and inequities in the urban canopy.\nBy combining spatial data with tree condition indicators, we will explore the data. The final part of this project proposes a targeted tree initiative that addresses two invasive species problems.\nThe following maps show NYC districts and tree coverage wihin.\n\n\nCode\nlibrary(sf)\nlibrary(dplyr)\n\nget_nyc_council_districts &lt;- function() {\n  dir.create(\"data/mp03\", showWarnings = FALSE, recursive = TRUE)\n  \n  url &lt;- \"https://s-media.nyc.gov/agencies/dcp/assets/files/zip/data-tools/bytes/city-council/nycc_25c.zip\"\n  zip_path &lt;- \"data/mp03/nycc_25c.zip\"\n  unzip_dir &lt;- \"data/mp03/nycc_25c\"\n  \n  if (!file.exists(zip_path)) {\n    download.file(url, destfile = zip_path, mode = \"wb\", method = \"libcurl\")\n  }\n  \n  if (!dir.exists(unzip_dir)) {\n    unzip(zip_path, exdir = unzip_dir)\n  }\n  \n  shp_file &lt;- list.files(unzip_dir, pattern = \"\\\\.shp$\", full.names = TRUE, recursive = TRUE)\n  if (length(shp_file) == 0) stop(\"No .shp file found after unzipping.\")\n  \n  nycc &lt;- sf::st_read(shp_file[1], quiet = TRUE)\n  nycc_wgs84 &lt;- sf::st_transform(nycc, crs = \"WGS84\")\n  return(nycc_wgs84)\n}\n\n# Call the function and simplify geometry\nnycc_districts &lt;- get_nyc_council_districts()\nnycc_districts_simplified &lt;- nycc_districts |&gt;\n  mutate(geometry = st_simplify(geometry, dTolerance = 30))\n\n# Optional: quick check\nplot(nycc_districts_simplified[\"CounDist\"], main = \"Simplified NYC Council Districts\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(httr2)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(glue)\n\nget_nyc_tree_points &lt;- function(limit = 50000) {\n  out_dir &lt;- \"data/mp03/trees\"\n  dir.create(out_dir, showWarnings = FALSE, recursive = TRUE)\n\n  base_url &lt;- \"https://data.cityofnewyork.us/resource/hn5i-inap.geojson\"\n  chunk &lt;- 1\n  done &lt;- FALSE\n\n  while (!done) {\n    file_path &lt;- file.path(out_dir, glue(\"trees_{limit}_{chunk}.geojson\"))\n    offset &lt;- (chunk - 1) * limit\n\n    # Only download if file doesn't already exist\n    if (!file.exists(file_path)) {\n      message(glue(\"Downloading chunk {chunk} (offset = {offset})...\"))\n      resp &lt;- request(base_url) |&gt;\n        req_url_query(`$limit` = limit, `$offset` = offset) |&gt;\n        req_perform()\n\n      # Save raw GeoJSON to disk\n      writeBin(resp_body_raw(resp), file_path)\n    } else {\n      message(glue(\"Chunk {chunk} already exists. Skipping download.\"))\n    }\n\n    # Check if this is the last chunk\n    tree_data &lt;- st_read(file_path, quiet = TRUE)\n    n &lt;- nrow(tree_data)\n    message(glue(\"Chunk {chunk} has {n} rows.\"))\n\n    if (n &lt; limit) {\n      done &lt;- TRUE\n      message(\"All tree point data downloaded \")\n    } else {\n      chunk &lt;- chunk + 1\n    }\n  }\n\n  # Combine all saved chunks into one sf object\n  all_files &lt;- list.files(out_dir, pattern = \"\\\\.geojson$\", full.names = TRUE)\n  all_data &lt;- all_files |&gt;\n    lapply(st_read, quiet = TRUE) |&gt;\n    bind_rows()\n\n  message(\"NYC Tree Points successfully combined \")\n  return(all_data)\n}\n\n# combined tree data:\nnyc_trees &lt;- get_nyc_tree_points(limit = 50000)\n\n# Save to disk so it doesn't need to be recombined next time\nsaveRDS(nyc_trees, \"data/mp03/nyc_tree_points_combined.rds\")\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(sf)\n\nggplot() +\n  # Plot ALL tree points first (as a background layer)\n  geom_sf(data = nyc_trees, color = \"darkgreen\", size = 0.05, alpha = 0.4) +\n  \n  # Plot district boundaries on top\n  geom_sf(data = nycc_districts_simplified, fill = NA, color = \"black\", linewidth = 0.3) +\n  \n  labs(\n    title = \"NYC Tree Points with Council District Boundaries Overlay\",\n    subtitle = \"Districts drawn on top of tree data for clarity\",\n    caption = \"Data sources: NYC Open Data (Trees), NYC DCP (Districts)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# make sure that both are sf objects and in the same CRS\nst_crs(nyc_trees) == st_crs(nycc_districts_simplified)\n\n\n[1] TRUE\n\n\nCode\n# Join tree points to the district they fall in\ntrees_with_districts &lt;- st_join(\n  nyc_trees,\n  nycc_districts_simplified[, c(\"CounDist\")], # Keep only district ID\n  join = st_intersects       \n)"
  },
  {
    "objectID": "mp03.html#data-exploration",
    "href": "mp03.html#data-exploration",
    "title": "Mini Project 03",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nWhich council district has the most trees?\n\nDistrict 51, in Staten Island, has the greatest number of trees.\n\n\nCode\nlibrary(dplyr)\nlibrary(DT)\nlibrary(scales)\nlibrary(htmltools)\n\n# Count trees per district\ntree_counts &lt;- trees_with_districts |&gt;\n  st_drop_geometry() |&gt;\n  group_by(CounDist) |&gt;\n  summarise(Tree_Count = n()) |&gt;\n  arrange(desc(Tree_Count))\n\n# Format numbers and column names\ntree_counts_formatted &lt;- tree_counts |&gt;\n  mutate(Tree_Count = comma(Tree_Count)) |&gt;\n  rename(`Council District` = CounDist, `Tree Count` = Tree_Count)\n\n# Interactive table: Top 5\ndatatable(\n  tree_counts_formatted |&gt; head(5),\n  rownames = FALSE,\n  caption = htmltools::tags$caption(\n    style = \"caption-side: top; text-align: center; font-weight: bold; color: #333; font-size: 16px;\",\n    \"Top 5 NYC Council Districts by Tree Count\"\n  ),\n  options = list(\n    pageLength = 5,\n    autoWidth = TRUE,\n    columnDefs = list(\n      list(className = 'dt-left', targets = \"_all\")\n    ),\n    dom = 't',\n    scrollX = FALSE,\n    compact = TRUE\n  ),\n  class = 'compact stripe hover row-border order-column',\n  style = \"bootstrap\"\n)\n\n\n\n\n\n\nCode\n#\n#\n#\n#\n#\n\n\n\nWhich council district has the highest density of trees?\n\nDistrict 7 in Manhattan has the highest tree density.\n\n\nCode\n# Extract district area\ndistrict_areas &lt;- nycc_districts_simplified |&gt;\n  st_drop_geometry() |&gt;\n  select(CounDist, Shape_Area)\n\n# Compute density (trees per sq km)\ndensity_data &lt;- tree_counts |&gt;\n  inner_join(district_areas, by = \"CounDist\") |&gt;\n  mutate(Tree_Density_per_SqKm = Tree_Count / (Shape_Area / 1e6)) |&gt;\n  arrange(desc(Tree_Density_per_SqKm))\n\n# Format numbers and column names\ndensity_data_formatted &lt;- density_data |&gt;\n  mutate(\n    Tree_Count = comma(Tree_Count),\n    Tree_Density_per_SqKm = comma(round(Tree_Density_per_SqKm, 1))\n  ) |&gt;\n  rename(\n    `Council District` = CounDist,\n    `Tree Count` = Tree_Count,\n    `Tree Density per SqKm` = Tree_Density_per_SqKm\n  )\n\n# Interactive table: Top 5\ndatatable(\n  density_data_formatted |&gt; head(5),\n  rownames = FALSE,\n  caption = htmltools::tags$caption(\n    style = \"caption-side: top; text-align: center; font-weight: bold; color: #333; font-size: 16px;\",\n    \"Top 5 NYC Council Districts by Tree Density (trees per sq km)\"\n  ),\n  options = list(\n    pageLength = 5,\n    autoWidth = TRUE,\n    columnDefs = list(\n      list(className = 'dt-left', targets = \"_all\")\n    ),\n    dom = 't',\n    scrollX = FALSE,\n    compact = TRUE\n  ),\n  class = 'compact stripe hover row-border order-column',\n  style = \"bootstrap\"\n)\n\n\n\n\n\n\nCode\n#\n#\n#\n#\n#\n\n\n\nWhich district has highest fraction of dead trees out of all trees?\n\nDistrict 32 has the highest fraction of dead trees.\n\n\nCode\n# Count dead trees per district\ndead_tree_counts &lt;- trees_with_districts |&gt;\n  st_drop_geometry() |&gt;\n  filter(tpcondition == \"Dead\") |&gt;\n  group_by(CounDist) |&gt;\n  summarise(Dead_Trees = n())\n\n# Compute fraction of dead trees\ndead_fraction &lt;- tree_counts |&gt;\n  inner_join(dead_tree_counts, by = \"CounDist\") |&gt;\n  mutate(Fraction_Dead = Dead_Trees / Tree_Count) |&gt;\n  arrange(desc(Fraction_Dead))\n\n# Format numbers and column names\ndead_fraction_formatted &lt;- dead_fraction |&gt;\n  mutate(\n    Tree_Count = comma(Tree_Count),\n    Dead_Trees = comma(Dead_Trees),\n    Fraction_Dead = percent(Fraction_Dead, accuracy = 0.1)\n  ) |&gt;\n  rename(\n    `Council District` = CounDist,\n    `Total Trees` = Tree_Count,\n    `Dead Trees` = Dead_Trees,\n    `Percent Dead` = Fraction_Dead\n  )\n\n# Interactive table: Top 5\ndatatable(\n  dead_fraction_formatted |&gt; head(5),\n  rownames = FALSE,\n  caption = htmltools::tags$caption(\n    style = \"caption-side: top; text-align: center; font-weight: bold; color: #333; font-size: 16px;\",\n    \"Top 5 NYC Council Districts by Percent of Dead Trees\"\n  ),\n  options = list(\n    pageLength = 5,\n    autoWidth = TRUE,\n    columnDefs = list(\n      list(className = 'dt-left', targets = \"_all\")\n    ),\n    dom = 't',\n    scrollX = FALSE,\n    compact = TRUE\n  ),\n  class = 'compact stripe hover row-border order-column',\n  style = \"bootstrap\"\n)\n\n\n\n\n\n\nCode\n#\n#\n#\n#\n#\n#\n\n\n\nWhat is the most common tree species in Manhattan?\n\nThe Gleditsia triacanthos var. inermis - Thornless honeylocust is the most common tree species in Manhattan\n\n\nCode\nlibrary(dplyr)\nlibrary(DT)\nlibrary(scales)\nlibrary(htmltools)\n\n# Add borough column based on council district\ntrees_with_borough &lt;- trees_with_districts |&gt;\n  mutate(\n    Borough = case_when(\n      CounDist &gt;= 1 & CounDist &lt;= 10 ~ \"Manhattan\",\n      CounDist &gt;= 11 & CounDist &lt;= 18 ~ \"Bronx\",\n      CounDist &gt;= 19 & CounDist &lt;= 32 ~ \"Queens\",\n      CounDist &gt;= 33 & CounDist &lt;= 48 ~ \"Brooklyn\",\n      CounDist &gt;= 49 & CounDist &lt;= 51 ~ \"Staten Island\",\n      TRUE ~ NA_character_\n    )\n  )\n\n# Filter for Manhattan and count tree species\nmanhattan_species &lt;- trees_with_borough |&gt;\n  filter(Borough == \"Manhattan\") |&gt;\n  st_drop_geometry() |&gt;\n  group_by(genusspecies) |&gt;\n  summarise(Count = n()) |&gt;\n  arrange(desc(Count))\n\n# Format results and pick top 5\nmanhattan_species_formatted &lt;- manhattan_species |&gt;\n  mutate(Count = comma(Count)) |&gt;\n  rename(`Tree Species` = genusspecies)\n\n# Interactive table of top 5 species\ndatatable(\n  manhattan_species_formatted |&gt; head(5),\n  rownames = FALSE,\n  caption = htmltools::tags$caption(\n    style = \"caption-side: top; text-align: center; font-weight: bold; color: #333; font-size: 16px;\",\n    \"Top 5 Tree Species in Manhattan\"\n  ),\n  options = list(\n    pageLength = 5,\n    autoWidth = TRUE,\n    columnDefs = list(list(className = 'dt-left', targets = \"_all\")),\n    dom = 't'\n  )\n)\n\n\n\n\n\n\nCode\n#\n#\n#\n#\n#\n\n\n\nWhat is the species of the tree closest to Baruch’s campus?\n\nThe closest tree to Baruch is a Liquidambar styraciflua, a sweetgum tree. It is 19.48 meters away from Baruch.\n\n\nCode\nlibrary(sf)\nlibrary(dplyr)\nlibrary(units)\n\n# Function to create a spatial point\nnew_st_point &lt;- function(lat, lon) {\n  st_sfc(\n    st_point(c(lon, lat)),\n    crs = 4326  # WGS84 for input\n  )\n}\n\n# Create Baruch's location as a spatial point in WGS84\nbaruch_point &lt;- new_st_point(40.7403, -73.9833)\n\n# Transform both trees and the point to meters\ntrees_transformed &lt;- st_transform(trees_with_districts, 2263)\nbaruch_projected &lt;- st_transform(baruch_point, 2263)\n\n# Compute distance to Baruch and find the closest tree\nclosest_tree &lt;- trees_transformed |&gt;\n  mutate(distance = st_distance(geometry, baruch_projected)) |&gt;\n  slice_min(distance, n = 1) |&gt;\n  st_drop_geometry()\n\nclosest_tree_species &lt;- closest_tree$genusspecies\nclosest_tree_distance &lt;- set_units(closest_tree$distance, \"meters\")\n\n# Output\ncat(\"The closest tree to Baruch is a:\", closest_tree_species, \"\\n\")\ncat(\"Distance:\", round(closest_tree_distance, 2), \"\\n\")"
  },
  {
    "objectID": "mp03.html#native-resistance-a-spotted-lanternfly-reduction-plan",
    "href": "mp03.html#native-resistance-a-spotted-lanternfly-reduction-plan",
    "title": "Mini Project 03",
    "section": "Native Resistance: A Spotted Lanternfly Reduction Plan",
    "text": "Native Resistance: A Spotted Lanternfly Reduction Plan\nWithin the past 4 years, The Spotted Lanternfly has rapidly taken over New York City in the summer months. The Lanternfly is a pesky invasive species that devastates important crops such as grapes, apples and hardwoods. In addition to the economic damage, there are ecological threats.\n\nProposed Initiative\nWe aim to reduce Spotted Lanternfly habitat and promote planting native species while doing so. The Lanternfly’s preferred host tree is the Ailanthus Altissima (tree of heaven). This tree itself also happens to be an invasive species that threatens the native habitat.\nA study done by Penn State has found that the Lanternfly rarely feeds on Oaks and Conifers. Our project thus aims to reduce lanternfly habitat by\n\nRemoving Ailanthus Altissima and\nReplanting resilient native species such as - White Oak (Quercus Alba)\n\nSwamp White Oak (Quercus Bicolor)\nEastern Red Cedar ( Juniperus Virginiana)\nEastern White Pine (Pinus Strobus)\n\n\n\nMethodology\nFirst, we must determine which District has the highest population of the tree of heaven to determine the best candidate for our pilot program. Comparing the top 5 districts with high populations of the tree of heaven, we find that district 50 has the highest population with 363 trees.\n\n\nCode\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(DT)\nlibrary(scales)\n\n# Filter for Tree of Heaven species\ntree_of_heaven &lt;- trees_with_districts |&gt;\n  st_drop_geometry() |&gt;\n  filter(str_detect(genusspecies, regex(\"Ailanthus altissima|tree of heaven\", ignore_case = TRUE)))\n\n# Count by district\ntree_of_heaven_counts &lt;- tree_of_heaven |&gt;\n  group_by(CounDist) |&gt;\n  summarise(TreeOfHeaven_Count = n()) |&gt;\n  arrange(desc(TreeOfHeaven_Count))\n\n# Format and rename columns\ntree_of_heaven_formatted &lt;- tree_of_heaven_counts |&gt;\n  mutate(TreeOfHeaven_Count = comma(TreeOfHeaven_Count)) |&gt;\n  rename(\n    `Council District` = CounDist,\n    `Tree of Heaven Count` = TreeOfHeaven_Count\n  )\n\n# Interactive table: Top 5 with styling updates\ndatatable(\n  tree_of_heaven_formatted |&gt; head(5),\n  rownames = FALSE,\n  caption = htmltools::tags$caption(\n    style = \"caption-side: top; text-align: center; font-weight: bold; color: #333;\",\n    \"Top 5 NYC Council Districts by Tree of Heaven Count\"\n  ),\n  options = list(\n    dom = 't',                \n    pageLength = 5,          \n    ordering = FALSE,         \n    columnDefs = list(\n      list(className = 'dt-left', targets = \"_all\")  # Left-align columns\n    )\n  ),\n  class = 'cell-border stripe'  \n)\n\n\n\n\n\n\nCode\n#\n#\n\n\nThe Following graphic shows the locations of all Ailanthus Altissima in District 50.\n\n\nCode\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Filter District 50\ndistrict_50 &lt;- nycc_districts_simplified |&gt;\n  filter(CounDist == 50) |&gt;\n  st_transform(4326)\n\n# Filter Trees of Heaven only and transform CRS\ntrees_of_heaven_dist50 &lt;- trees_with_districts |&gt;\n  filter(\n    CounDist == 50,\n    grepl(\"Ailanthus altissima|tree of heaven\", genusspecies, ignore.case = TRUE)\n  ) |&gt;\n  st_transform(4326)\n\n# Check if any Tree of Heaven found\nif (nrow(trees_of_heaven_dist50) == 0) {\n  message(\"No Tree of Heaven found in District 50.\")\n} else {\n  # Plot map\n  ggplot() +\n    geom_sf(data = district_50, fill = \"white\", color = \"black\") +\n    geom_sf(data = trees_of_heaven_dist50, color = \"red\", size = 1, alpha = 0.6) +\n    labs(\n      title = \"Tree of Heaven Locations in NYC Council District 50\",\n      subtitle = \"District boundary shown, Tree of Heaven marked as red dots\"\n    ) +\n    theme_minimal()\n}\n\n\n\n\n\n\n\n\n\nThere area 363 Ailanthus altissima trees in district 50. Our pilot program will remove all of these trees and replace then with the four native species in consultation with NYC Parks and their ecological expertise. We will closely monitor lantern fly populations in the distrcit before and after the pilot program.\n\n\nPredicted Impact\nThe initiative, if successful, will\n\nRestore native habitats\nIncrease native New York plant life\nReduce invasive plant populations\nReduce invasive insect populations\n\nThis pilot program could make a huge difference in the NYC ecological landscape. It is imperative that we mitigate invasive species as soon as possible. The longer that these species are allowed to thrive, the longer they wreak havok and cause irreperable damage."
  },
  {
    "objectID": "mp03.html#introduction",
    "href": "mp03.html#introduction",
    "title": "Mini Project 03",
    "section": "",
    "text": "New York City’s street trees play a crucial role in shaping the city’s environment and public spaces, but they also face challenges ranging from invasive species to uneven maintenance across neighborhoods. In this project, we will analyze the NYC TreeMap data, which catalogs tree species, health, and location across all council districts, to uncover patterns and inequities in the urban canopy.\nBy combining spatial data with tree condition indicators, we will explore the data. The final part of this project proposes a targeted tree initiative that addresses two invasive species problems.\nThe following maps show NYC districts and tree coverage wihin.\n\n\nCode\nlibrary(sf)\nlibrary(dplyr)\n\nget_nyc_council_districts &lt;- function() {\n  dir.create(\"data/mp03\", showWarnings = FALSE, recursive = TRUE)\n  \n  url &lt;- \"https://s-media.nyc.gov/agencies/dcp/assets/files/zip/data-tools/bytes/city-council/nycc_25c.zip\"\n  zip_path &lt;- \"data/mp03/nycc_25c.zip\"\n  unzip_dir &lt;- \"data/mp03/nycc_25c\"\n  \n  if (!file.exists(zip_path)) {\n    download.file(url, destfile = zip_path, mode = \"wb\", method = \"libcurl\")\n  }\n  \n  if (!dir.exists(unzip_dir)) {\n    unzip(zip_path, exdir = unzip_dir)\n  }\n  \n  shp_file &lt;- list.files(unzip_dir, pattern = \"\\\\.shp$\", full.names = TRUE, recursive = TRUE)\n  if (length(shp_file) == 0) stop(\"No .shp file found after unzipping.\")\n  \n  nycc &lt;- sf::st_read(shp_file[1], quiet = TRUE)\n  nycc_wgs84 &lt;- sf::st_transform(nycc, crs = \"WGS84\")\n  return(nycc_wgs84)\n}\n\n# Call the function and simplify geometry\nnycc_districts &lt;- get_nyc_council_districts()\nnycc_districts_simplified &lt;- nycc_districts |&gt;\n  mutate(geometry = st_simplify(geometry, dTolerance = 30))\n\n# Optional: quick check\nplot(nycc_districts_simplified[\"CounDist\"], main = \"Simplified NYC Council Districts\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(httr2)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(glue)\n\nget_nyc_tree_points &lt;- function(limit = 50000) {\n  out_dir &lt;- \"data/mp03/trees\"\n  dir.create(out_dir, showWarnings = FALSE, recursive = TRUE)\n\n  base_url &lt;- \"https://data.cityofnewyork.us/resource/hn5i-inap.geojson\"\n  chunk &lt;- 1\n  done &lt;- FALSE\n\n  while (!done) {\n    file_path &lt;- file.path(out_dir, glue(\"trees_{limit}_{chunk}.geojson\"))\n    offset &lt;- (chunk - 1) * limit\n\n    # Only download if file doesn't already exist\n    if (!file.exists(file_path)) {\n      message(glue(\"Downloading chunk {chunk} (offset = {offset})...\"))\n      resp &lt;- request(base_url) |&gt;\n        req_url_query(`$limit` = limit, `$offset` = offset) |&gt;\n        req_perform()\n\n      # Save raw GeoJSON to disk\n      writeBin(resp_body_raw(resp), file_path)\n    } else {\n      message(glue(\"Chunk {chunk} already exists. Skipping download.\"))\n    }\n\n    # Check if this is the last chunk\n    tree_data &lt;- st_read(file_path, quiet = TRUE)\n    n &lt;- nrow(tree_data)\n    message(glue(\"Chunk {chunk} has {n} rows.\"))\n\n    if (n &lt; limit) {\n      done &lt;- TRUE\n      message(\"All tree point data downloaded \")\n    } else {\n      chunk &lt;- chunk + 1\n    }\n  }\n\n  # Combine all saved chunks into one sf object\n  all_files &lt;- list.files(out_dir, pattern = \"\\\\.geojson$\", full.names = TRUE)\n  all_data &lt;- all_files |&gt;\n    lapply(st_read, quiet = TRUE) |&gt;\n    bind_rows()\n\n  message(\"NYC Tree Points successfully combined \")\n  return(all_data)\n}\n\n# combined tree data:\nnyc_trees &lt;- get_nyc_tree_points(limit = 50000)\n\n# Save to disk so it doesn't need to be recombined next time\nsaveRDS(nyc_trees, \"data/mp03/nyc_tree_points_combined.rds\")\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(sf)\n\nggplot() +\n  # Plot ALL tree points first (as a background layer)\n  geom_sf(data = nyc_trees, color = \"darkgreen\", size = 0.05, alpha = 0.4) +\n  \n  # Plot district boundaries on top\n  geom_sf(data = nycc_districts_simplified, fill = NA, color = \"black\", linewidth = 0.3) +\n  \n  labs(\n    title = \"NYC Tree Points with Council District Boundaries Overlay\",\n    subtitle = \"Districts drawn on top of tree data for clarity\",\n    caption = \"Data sources: NYC Open Data (Trees), NYC DCP (Districts)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# make sure that both are sf objects and in the same CRS\nst_crs(nyc_trees) == st_crs(nycc_districts_simplified)\n\n\n[1] TRUE\n\n\nCode\n# Join tree points to the district they fall in\ntrees_with_districts &lt;- st_join(\n  nyc_trees,\n  nycc_districts_simplified[, c(\"CounDist\")], # Keep only district ID\n  join = st_intersects       \n)"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "STA 9750 — Mini-Project 04",
    "section": "",
    "text": "Every month, the Bureau of Labor Statistics releases the jobs report: one number that can move the stock market, shift interest rate expectations, and shape the political conversation overnight. Because so much rides on it, the accuracy of the jobs number matters. When President Trump fired BLS Commissioner Erika McEntarfer in 2025, arguing that the agency had produced “inflated” job figures, the decision immediately set off a national debate.\nThis project takes a step back from the politics and looks directly at the data behind that debate. Using employment levels and revision tables from 1979 to 2025, we examine how often the initial jobs numbers are revised, how large those revisions tend to be, and whether any patterns line up with the political claims being made. We summarize long-run trends, build visualizations, and run statistical tests to see what the evidence actually shows."
  },
  {
    "objectID": "mp04.html#fetch-revision-page",
    "href": "mp04.html#fetch-revision-page",
    "title": "STA 9750 — Mini-Project 04",
    "section": "Fetch revision page",
    "text": "Fetch revision page\n\n\nCode\nlibrary(httr2)\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(purrr)\nlibrary(tibble)\n\n# Revisions request (you already have this earlier; keep it once)\nrev_req &lt;- request(\"https://www.bls.gov\") |&gt;\n  req_url_path(\"web\", \"empsit\", \"cesnaicsrev.htm\") |&gt;\n  req_headers(\n    `User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\"\n  )\n\nrev_resp &lt;- rev_req |&gt;\n  req_perform()\n\nrev_page &lt;- rev_resp |&gt;\n  resp_body_html()\n\n# Helper: extract CES revisions for a single year table\nget_ces_revisions_year &lt;- function(yr, page = rev_page) {\n  # Find the table with id equal to the year; if it does not exist, return empty\n  year_node &lt;- page |&gt;\n    html_element(paste0(\"#\", yr))\n  \n  if (is.null(year_node)) {\n    # Return a 0-row tibble with the correct columns so list_rbind() works\n    return(tibble(\n      date     = as.Date(character()),\n      original = integer(),\n      final    = integer(),\n      revision = integer()\n    ))\n  }\n  \n  year_tbl &lt;- year_node |&gt;\n    html_element(\"tbody\") |&gt;\n    html_table(header = FALSE)\n  \n  year_tbl |&gt;\n    as_tibble() |&gt;\n    # First 12 rows are Jan–Dec\n    slice(1:12) |&gt;\n    # Coerce numeric columns safely with as.integer()\n    transmute(\n      month    = X1,\n      year     = as.integer(X2),\n      original = as.integer(X3),\n      final    = as.integer(X5),\n      revision = as.integer(X8),\n      month_clean = stringr::str_replace(month, \"\\\\.\", \"\"),\n      date        = lubridate::ym(paste(year, month_clean))\n    ) |&gt;\n    select(date, original, final, revision) |&gt;\n    filter(!is.na(date))\n}\n\n# Quick sanity checks for a couple of years\nget_ces_revisions_year(2024)\nget_ces_revisions_year(2023) |&gt; head()\n\n# Build the full revisions data set\nyears_full &lt;- 1979:2025\n\nces_revisions_raw &lt;- years_full |&gt;\n  map(get_ces_revisions_year) |&gt;\n  list_rbind() |&gt;\n  arrange(date)\n\n\nces_revisions &lt;- ces_revisions_raw |&gt;\n  filter(date &gt;= as.Date(\"1979-01-01\")) |&gt;\n  filter(date &lt;= as.Date(\"2025-06-01\")) |&gt;\n  arrange(date)\n\nhead(ces_revisions)\ntail(ces_revisions)\n\n\n\n\nCode\ndatatable(\n  ces_revisions,\n  rownames = FALSE,\n  options = list(\n    pageLength = 5,\n    autoWidth = TRUE,\n    dom = \"tip\",\n    columnDefs = list(list(className = \"dt-center\", targets = \"_all\"))\n  )\n) |&gt;\n  formatStyle(\n    columns = names(ces_revisions),\n    `text-align` = \"center\"\n  ) |&gt;\n  htmlwidgets::prependContent(\n    htmltools::tags$h3(\n      \"CES Nonfarm Payroll Revisions (Original vs Final, 1979–2025)\",\n      style = \"text-align:center; font-weight:medium; font-size:18px; color:#222; margin-bottom:20px;\"\n    )\n  )\n\n\nCES Nonfarm Payroll Revisions (Original vs Final, 1979–2025)"
  },
  {
    "objectID": "mp04.html#does-the-data-support-the-white-houses-claim",
    "href": "mp04.html#does-the-data-support-the-white-houses-claim",
    "title": "STA 9750 — Mini-Project 04",
    "section": "Does the Data Support the White House’s Claim?",
    "text": "Does the Data Support the White House’s Claim?\nThe Trump administration’s public justification for firing the Bureau of Labor Statistics Commissioner Erika McEntarfer makes a bold claim. As the White House put it, the BLS under her leadership has shown “a lengthy history of inaccuracies and incompetence”, and published “overly optimistic jobs numbers - only for those numbers to be quietly revised later.”\nTrump argues that Commissioner McEntarfer repeatedly published inflated preliminary job numbers during the Biden administration, making the economy appear stronger than it was. He further claims that she continued this pattern after he took office, now suggesting that overly optimistic job reports would discourage the Federal Reserve from cutting interest rates, a policy he publicly supports.\nThe White House claim suggests two things about the period under McEntarfer which started February 2024:\n\nLarge revisions are more common than in earlier decades.\nNegative revisions are also more common, implying that initial job estimates were systematically overstated.\n\nTo evaluate whether the data supports this narrative, we perform two statistical comparisons between the McEntarfer era (Feb 2024–Jun 2025) and the historical period (1979–Jan 2024):\nTest A: Is the share of substantial revisions (revisions greater than 50,000 jobs) significantly higher under McEntarfer?\nTest B: Is the share of negative revisions significantly higher under McEntarfer?\nIf either proportion shows a statistically significant increase, then the data would give empirical support to the White House’s claim.\nIf both changes are statistically insignificant, the historical revision record does not support the rationale for the dismissal.\n\nTest A: Is the share of substantial revisions more common under McEntarfer? \n\n\nCode\n# Test A: Is the share of substantial revisions more common under McEntarfer?\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(infer)\n\n# Define eras and a \"substantial revision\" indicator \nces_fc1 &lt;- ces_full_clean |&gt;\n  mutate(\n    era = if_else(\n      date &gt;= as.Date(\"2024-02-01\") & date &lt;= as.Date(\"2025-06-01\"),\n      \"McEntarfer\",\n      \"Earlier\"\n    ),\n    substantial = as.integer(abs(revision) &gt; 50)\n  )\n\n# Summary table (rounded to 3 decimals)\nfc1a_summary &lt;- ces_fc1 |&gt;\n  group_by(era) |&gt;\n  summarise(\n    Months              = n(),\n    FractionSubstantial = round(mean(substantial, na.rm = TRUE), 3),\n    .groups = \"drop\"\n  )\n\nmake_dt(\n  fc1a_summary,\n  title = \"Fraction of Substantial Revisions (&gt;|50,000| Jobs), Earlier vs McEntarfer\",\n  pageLength = 5\n)\n\n\nFraction of Substantial Revisions (&gt;|50,000| Jobs), Earlier vs McEntarfer\n\n\n\n\nCode\n# Two-sample Welch t-test \nfc1a_test &lt;- t_test(\n  ces_fc1,\n  substantial ~ era,\n  order = c(\"Earlier\", \"McEntarfer\")\n)\n\n# Display table rounded to 3 decimals\nfc1a_display &lt;- fc1a_test |&gt;\n  transmute(\n    Difference = round(estimate, 3),\n    T_Df       = round(t_df, 3),\n    P_Value    = signif(p_value, 3),\n    Lower_CI   = round(lower_ci, 3),\n    Upper_CI   = round(upper_ci, 3)\n  )\n\nmake_dt(\n  fc1a_display,\n  title = \"Test A: Are Substantial Revisions More Common in the McEntarfer Era?\",\n  pageLength = 5\n)\n\n\nTest A: Are Substantial Revisions More Common in the McEntarfer Era?\n\n\n\n\nCode\n# Save values for inline use\nfc1a_diff  &lt;- round(as.numeric(fc1a_test$estimate), 3)\nfc1a_p     &lt;- signif(as.numeric(fc1a_test$p_value), 3)\nfc1a_lower &lt;- round(as.numeric(fc1a_test$lower_ci), 3)\nfc1a_upper &lt;- round(as.numeric(fc1a_test$upper_ci), 3)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(scales)\n\n# Build dataset for substantial revisions only\nsubstantial_yearly &lt;- ces_full_clean |&gt;\n  mutate(\n    year = year(date),\n    substantial = abs(revision) &gt; 50,                # 50 = 50,000 jobs\n    era = if_else(\n      date &lt; as.Date(\"2024-02-01\"),\n      \"Earlier\",\n      \"McEntarfer\"\n    )\n  ) |&gt;\n  filter(substantial) |&gt;                              # keep only substantial revisions\n  group_by(year, era) |&gt;\n  summarise(\n    Count = n(),\n    .groups = \"drop\"\n  )\n\nggplot(substantial_yearly, aes(x = year, y = Count, fill = era)) +\n  geom_col() +\n  scale_fill_manual(\n    values = c(\n      \"Earlier\"     = \"#C92A2A\",  # red\n      \"McEntarfer\"  = \"#1971C2\"   # blue\n    )\n  ) +\n  labs(\n    title = \"Number of Substantial CES Revisions per Year\",\n    subtitle = \"Substantial revisions defined as |revision| &gt; 50,000 jobs\",\n    x = \"Year\",\n    y = \"Count of Substantial Revisions\",\n    fill = \"Era\"\n  ) +\n  scale_x_continuous(breaks = seq(1980, 2025, 5)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title    = element_text(face = \"bold\"),\n    plot.subtitle = element_text(),\n    axis.text.x   = element_text(angle = 90, vjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nTo evaluate whether substantial revisions were more common under Commissioner McEntarfer, we tested the hypotheses:\nH₀: The proportion of substantial revisions is the same in the McEntarfer era and the earlier period.\nH₁: The proportion of substantial revisions is different between the two periods.\nThe estimated difference in proportions (McEntarfer minus Earlier) was −0.058, meaning the McEntarfer era actually had a slightly lower share of substantial revisions. The 95% confidence interval ranged from −0.326 to 0.209, which includes zero. This indicates that the true difference could reasonably be negative, zero, or positive.\nThe p-value for this test was 0.65, which is far greater than the conventional 0.05 threshold. Because the p-value is so large, we fail to reject the null hypothesis. In practical terms, there is no statistical evidence that substantial revisions became more common under McEntarfer. The small observed difference is well within the level of variation that would be expected by random chance.\nAdditionally, the short tenure of McEntarfer lends to a very small sample size to test on for that era.\n\n\nTest B: Are negative revisions more common under McEntarfer? \n\n\nCode\n# Test B: Are negative revisions more common under McEntarfer?\n\n\nces_fc2 &lt;- ces_full_clean |&gt;\n  mutate(\n    era = if_else(\n      date &gt;= as.Date(\"2024-02-01\") & date &lt;= as.Date(\"2025-06-01\"),\n      \"McEntarfer\",\n      \"Earlier\"\n    ),\n    negative_rev = as.integer(revision &lt; 0)\n  )\n\n# Summary table (rounded to 3 decimals)\nfc2b_summary &lt;- ces_fc2 |&gt;\n  group_by(era) |&gt;\n  summarise(\n    Months           = n(),\n    FractionNegative = round(mean(negative_rev, na.rm = TRUE), 3),\n    .groups = \"drop\"\n  )\n\nmake_dt(\n  fc2b_summary,\n  title = \"Fraction of Negative Revisions, Earlier vs McEntarfer\",\n  pageLength = 5\n)\n\n\nFraction of Negative Revisions, Earlier vs McEntarfer\n\n\n\n\nCode\n# Welch t-test on the 0/1 indicator\nfc2b_test &lt;- t_test(\n  ces_fc2,\n  negative_rev ~ era,\n  order = c(\"Earlier\", \"McEntarfer\")\n)\n\n# Display table rounded to 3 decimals\nfc2b_display &lt;- fc2b_test |&gt;\n  transmute(\n    Difference = round(estimate, 3),\n    T_Df       = round(t_df, 3),\n    P_Value    = signif(p_value, 3),\n    Lower_CI   = round(lower_ci, 3),\n    Upper_CI   = round(upper_ci, 3)\n  )\n\nmake_dt(\n  fc2b_display,\n  title = \"Test B: Are Negative Revisions More Common in the McEntarfer Era?\",\n  pageLength = 5\n)\n\n\nTest B: Are Negative Revisions More Common in the McEntarfer Era?\n\n\n\n\nCode\n# Save values for inline use\nfc2b_diff  &lt;- round(as.numeric(fc2b_test$estimate), 3)\nfc2b_p     &lt;- signif(as.numeric(fc2b_test$p_value), 3)\nfc2b_lower &lt;- round(as.numeric(fc2b_test$lower_ci), 3)\nfc2b_upper &lt;- round(as.numeric(fc2b_test$upper_ci), 3)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(lubridate)\n\n# Add an era variable for coloring\nces_rev_era &lt;- ces_full |&gt;\n  mutate(\n    era = if_else(\n      date &lt; as.Date(\"2024-02-01\"),\n      \"Earlier\",\n      \"McEntarfer\"\n    )\n  )\n\nggplot(ces_rev_era, aes(x = date, y = revision, color = era)) +\n  geom_line() +\n  geom_hline(yintercept = 0, linewidth = 0.4) +\n  scale_color_manual(\n    values = c(\n      \"Earlier\"     = \"#C92A2A\",  # red\n      \"McEntarfer\"  = \"#1971C2\"   # blue\n    )\n  ) +\n  labs(\n    title = \"CES Revisions Over Time (Earlier vs McEntarfer Period)\",\n    subtitle = \"Revisions shown as final minus original estimates\",\n    x = \"Year\",\n    y = \"Revision in Employment Level (thousands of jobs)\",\n    color = \"Era\"\n  ) +\n  scale_y_continuous(labels = comma) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title    = element_text(face = \"bold\"),\n    plot.subtitle = element_text(),\n    axis.title    = element_text()\n  )\n\n\n\n\n\n\n\n\n\nTo evaluate whether negative revisions were more common under Commissioner McEntarfer, we tested the hypotheses:\nH₀: The proportion of negative revisions is the same in the McEntarfer era and the earlier period.\nH₁: The proportions differ between the two periods.\nThe earlier period had a negative revision rate of 0.418, compared with 0.647 under McEntarfer, a raw increase of about 23 percentage points. The estimated difference in proportions (McEntarfer minus Earlier) was –0.229, and the 95% confidence interval ranged from –0.485 to 0.027. Because this interval includes zero, the data do not rule out the possibility that the true difference is actually zero or even slightly positive.\nThe p-value for the test was 0.0759, which is above 0.05. Since the p-value is not small enough to reject the null hypothesis, we conclude that there is not sufficient statistical evidence that negative revisions became more common under McEntarfer. Although the observed difference is large in magnitude, the limited number of months in the McEntarfer period (n = 17) means the estimate is imprecise, and the result is not statistically significant at the 5% level.\nCitation: White House. “BLS Has Lengthy History of Inaccuracies, Incompetence.” whitehouse.gov, August 1, 2025. https://www.whitehouse.gov/articles/2025/08/bls-has-lengthy-history-of-inaccuracies-incompetence/"
  },
  {
    "objectID": "mp04.html#claim-1-does-the-data-support-the-white-houses-claim",
    "href": "mp04.html#claim-1-does-the-data-support-the-white-houses-claim",
    "title": "STA 9750 — Mini-Project 04",
    "section": "Claim 1: Does the Data Support the White House’s Claim?",
    "text": "Claim 1: Does the Data Support the White House’s Claim?\nThe Trump administration’s public justification for firing the Bureau of Labor Statistics Commissioner Erika McEntarfer makes a bold claim. As the White House put it, the BLS under her leadership has shown “a lengthy history of inaccuracies and incompetence”, and published “overly optimistic jobs numbers - only for those numbers to be quietly revised later.”\nTrump argues that Commissioner McEntarfer repeatedly published inflated preliminary job numbers during the Biden administration, making the economy appear stronger than it was. He further claims that she continued this pattern after he took office, now suggesting that overly optimistic job reports would discourage the Federal Reserve from cutting interest rates, a policy he publicly supports.\nThe White House claim suggests two things about the period under McEntarfer which started February 2024:\n\nLarge revisions are more common than in earlier decades.\nNegative revisions are also more common, implying that initial job estimates were systematically overstated.\n\nTo evaluate whether the data supports this narrative, we perform two statistical comparisons between the McEntarfer era (Feb 2024–Jun 2025) and the historical period (1979–Jan 2024):\nTest A: Is the share of substantial revisions (revisions greater than 50,000 jobs) significantly higher under McEntarfer?\nTest B: Is the share of negative revisions significantly higher under McEntarfer?\nIf either proportion shows a statistically significant increase, then the data would give empirical support to the White House’s claim.\nIf both changes are statistically insignificant, the historical revision record does not support the rationale for the dismissal.\n\nTest A: Is the share of substantial revisions more common under McEntarfer? \n\n\nCode\n# Test A: Is the share of substantial revisions more common under McEntarfer?\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(infer)\n\n# Define eras and a \"substantial revision\" indicator (threshold: 50 = 50,000 jobs)\nces_fc1 &lt;- ces_full_clean |&gt;\n  mutate(\n    era = if_else(\n      date &gt;= as.Date(\"2024-02-01\") & date &lt;= as.Date(\"2025-06-01\"),\n      \"McEntarfer\",\n      \"Earlier\"\n    ),\n    substantial = as.integer(abs(revision) &gt; 50)\n  )\n\n# Summary table (rounded to 3 decimals)\nfc1a_summary &lt;- ces_fc1 |&gt;\n  group_by(era) |&gt;\n  summarise(\n    Months              = n(),\n    FractionSubstantial = round(mean(substantial, na.rm = TRUE), 3),\n    .groups = \"drop\"\n  )\n\nmake_dt(\n  fc1a_summary,\n  title = \"Fraction of Substantial Revisions (&gt;|50,000| Jobs), Earlier vs McEntarfer\",\n  pageLength = 5\n)\n\n\nFraction of Substantial Revisions (&gt;|50,000| Jobs), Earlier vs McEntarfer\n\n\n\n\nCode\n# Two-sample Welch t-test on the 0/1 indicator\nfc1a_test &lt;- t_test(\n  ces_fc1,\n  substantial ~ era,\n  order = c(\"Earlier\", \"McEntarfer\")\n)\n\n# Display table rounded to 3 decimals\nfc1a_display &lt;- fc1a_test |&gt;\n  transmute(\n    Difference = round(estimate, 3),\n    T_Df       = round(t_df, 3),\n    P_Value    = signif(p_value, 3),\n    Lower_CI   = round(lower_ci, 3),\n    Upper_CI   = round(upper_ci, 3)\n  )\n\nmake_dt(\n  fc1a_display,\n  title = \"Test A: Are Substantial Revisions More Common in the McEntarfer Era?\",\n  pageLength = 5\n)\n\n\nTest A: Are Substantial Revisions More Common in the McEntarfer Era?\n\n\n\n\nCode\n# Save values for inline use\nfc1a_diff  &lt;- round(as.numeric(fc1a_test$estimate), 3)\nfc1a_p     &lt;- signif(as.numeric(fc1a_test$p_value), 3)\nfc1a_lower &lt;- round(as.numeric(fc1a_test$lower_ci), 3)\nfc1a_upper &lt;- round(as.numeric(fc1a_test$upper_ci), 3)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(scales)\n\n# Build dataset for substantial revisions only\nsubstantial_yearly &lt;- ces_full_clean |&gt;\n  mutate(\n    year = year(date),\n    substantial = abs(revision) &gt; 50,                # 50 = 50,000 jobs\n    era = if_else(\n      date &lt; as.Date(\"2024-02-01\"),\n      \"Earlier\",\n      \"McEntarfer\"\n    )\n  ) |&gt;\n  filter(substantial) |&gt;                              # keep only substantial revisions\n  group_by(year, era) |&gt;\n  summarise(\n    Count = n(),\n    .groups = \"drop\"\n  )\n\nggplot(substantial_yearly, aes(x = year, y = Count, fill = era)) +\n  geom_col() +\n  scale_fill_manual(\n    values = c(\n      \"Earlier\"     = \"#C92A2A\",  # red\n      \"McEntarfer\"  = \"#1971C2\"   # blue\n    )\n  ) +\n  labs(\n    title = \"Number of Substantial CES Revisions per Year\",\n    subtitle = \"Substantial revisions defined as |revision| &gt; 50,000 jobs\",\n    x = \"Year\",\n    y = \"Count of Substantial Revisions\",\n    fill = \"Era\"\n  ) +\n  scale_x_continuous(breaks = seq(1980, 2025, 5)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title    = element_text(face = \"bold\"),\n    plot.subtitle = element_text(),\n    axis.text.x   = element_text(angle = 90, vjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nTo evaluate whether substantial revisions were more common under Commissioner McEntarfer, we tested the hypotheses:\nH₀: The proportion of substantial revisions is the same in the McEntarfer era and the earlier period.\nH₁: The proportion of substantial revisions is different between the two periods.\nThe estimated difference in proportions (McEntarfer minus Earlier) was −0.058, meaning the McEntarfer era actually had a slightly lower share of substantial revisions. The 95% confidence interval ranged from −0.326 to 0.209, which includes zero. This indicates that the true difference could reasonably be negative, zero, or positive.\nThe p-value for this test was 0.65, which is far greater than the conventional 0.05 threshold. Because the p-value is so large, we fail to reject the null hypothesis. In practical terms, there is no statistical evidence that substantial revisions became more common under McEntarfer. The small observed difference is well within the level of variation that would be expected by random chance.\nAdditionally, the short tenure of McEntarfer lends to a very small sample size to test on for that era.\n\n\nTest B: Are negative revisions more common under McEntarfer? \n\n\nCode\n# Test B: Are negative revisions more common under McEntarfer?\n\n\nces_fc2 &lt;- ces_full_clean |&gt;\n  mutate(\n    era = if_else(\n      date &gt;= as.Date(\"2024-02-01\") & date &lt;= as.Date(\"2025-06-01\"),\n      \"McEntarfer\",\n      \"Earlier\"\n    ),\n    negative_rev = as.integer(revision &lt; 0)\n  )\n\n# Summary table (rounded to 3 decimals)\nfc2b_summary &lt;- ces_fc2 |&gt;\n  group_by(era) |&gt;\n  summarise(\n    Months           = n(),\n    FractionNegative = round(mean(negative_rev, na.rm = TRUE), 3),\n    .groups = \"drop\"\n  )\n\nmake_dt(\n  fc2b_summary,\n  title = \"Fraction of Negative Revisions, Earlier vs McEntarfer\",\n  pageLength = 5\n)\n\n\nFraction of Negative Revisions, Earlier vs McEntarfer\n\n\n\n\nCode\n# Welch t-test on the 0/1 indicator\nfc2b_test &lt;- t_test(\n  ces_fc2,\n  negative_rev ~ era,\n  order = c(\"Earlier\", \"McEntarfer\")\n)\n\n# Display table rounded to 3 decimals\nfc2b_display &lt;- fc2b_test |&gt;\n  transmute(\n    Difference = round(estimate, 3),\n    T_Df       = round(t_df, 3),\n    P_Value    = signif(p_value, 3),\n    Lower_CI   = round(lower_ci, 3),\n    Upper_CI   = round(upper_ci, 3)\n  )\n\nmake_dt(\n  fc2b_display,\n  title = \"Test B: Are Negative Revisions More Common in the McEntarfer Era?\",\n  pageLength = 5\n)\n\n\nTest B: Are Negative Revisions More Common in the McEntarfer Era?\n\n\n\n\nCode\n# Save values for inline use\nfc2b_diff  &lt;- round(as.numeric(fc2b_test$estimate), 3)\nfc2b_p     &lt;- signif(as.numeric(fc2b_test$p_value), 3)\nfc2b_lower &lt;- round(as.numeric(fc2b_test$lower_ci), 3)\nfc2b_upper &lt;- round(as.numeric(fc2b_test$upper_ci), 3)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(lubridate)\n\n# Add an era variable for coloring\nces_rev_era &lt;- ces_full |&gt;\n  mutate(\n    era = if_else(\n      date &lt; as.Date(\"2024-02-01\"),\n      \"Earlier\",\n      \"McEntarfer\"\n    )\n  )\n\nggplot(ces_rev_era, aes(x = date, y = revision, color = era)) +\n  geom_line() +\n  geom_hline(yintercept = 0, linewidth = 0.4) +\n  scale_color_manual(\n    values = c(\n      \"Earlier\"     = \"#C92A2A\",  # red\n      \"McEntarfer\"  = \"#1971C2\"   # blue\n    )\n  ) +\n  labs(\n    title = \"CES Revisions Over Time (Earlier vs McEntarfer Period)\",\n    subtitle = \"Revisions shown as final minus original estimates\",\n    x = \"Year\",\n    y = \"Revision in Employment Level (thousands of jobs)\",\n    color = \"Era\"\n  ) +\n  scale_y_continuous(labels = comma) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title    = element_text(face = \"bold\"),\n    plot.subtitle = element_text(),\n    axis.title    = element_text()\n  )\n\n\n\n\n\n\n\n\n\nTo evaluate whether negative revisions were more common under Commissioner McEntarfer, we tested the hypotheses:\nH₀: The proportion of negative revisions is the same in the McEntarfer era and the earlier period.\nH₁: The proportions differ between the two periods.\nThe earlier period had a negative revision rate of 0.418, compared with 0.647 under McEntarfer, a raw increase of about 23 percentage points. The estimated difference in proportions (McEntarfer minus Earlier) was –0.229, and the 95% confidence interval ranged from –0.485 to 0.027. Because this interval includes zero, the data do not rule out the possibility that the true difference is actually zero or even slightly positive.\nThe p-value for the test was 0.0759, which is above 0.05. Since the p-value is not small enough to reject the null hypothesis, we conclude that there is not sufficient statistical evidence that negative revisions became more common under McEntarfer. Although the observed difference is large in magnitude, the limited number of months in the McEntarfer period (n = 17) means the estimate is imprecise, and the result is not statistically significant at the 5% level.\nCitation: White House. “BLS Has Lengthy History of Inaccuracies, Incompetence.” whitehouse.gov, August 1, 2025. https://www.whitehouse.gov/articles/2025/08/bls-has-lengthy-history-of-inaccuracies-incompetence/"
  },
  {
    "objectID": "mp04.html#does-the-data-support-the-claim-that-the-bls-favors-democratic-administrations",
    "href": "mp04.html#does-the-data-support-the-claim-that-the-bls-favors-democratic-administrations",
    "title": "STA 9750 — Mini-Project 04",
    "section": "Does the Data Support the Claim That the BLS Favors Democratic Administrations?",
    "text": "Does the Data Support the Claim That the BLS Favors Democratic Administrations?\nBeyond the debate over Commissioner McEntarfer’s dismissal, some (fictional) political commentators have argued that the Bureau of Labor Statistics tends to produce stronger employment numbers when a Democrat is in the White House.\nThe claim suggests that the CES estimates are somehow influenced by partisan considerations, leading to higher published job numbers and stronger perceived employment growth during Democratic administrations.\nAs one commentator put it, the BLS “always seems to report better job numbers for Democrats than Republicans” implying that the agency systematically favors one party over the other.\nThis claim implies two measurable expectations about the historical jobs data:\n\nEmployment levels should be higher under Democratic presidents because the BLS allegedly portrays stronger economic performance during Democratic administrations.\nRevisions under democratic administrations should be more often negative.\n\nTo evaluate whether the data support this narrative, we compare the full CES record from 1979 to 2025 across presidential administrations, classifying each month according to the president’s political party. This allows us to test whether differences in employment levels and job growth between Democratic and Republican presidencies are statistically meaningful or simply reflect long term economic trends such as population growth and business cycles. We perform two statistical comparisons between Democrat led months and Republican led months:\nTest C: Are CES employment levels significantly higher during Democratic presidencies?\nTest D: Are monthly CES revisions more often negative during Democratic presidencies?\nIf either test reveals statistically significant differences, that would provide empirical support for the claim that CES employment reporting favors Democratic administrations. If both differences are statistically insignificant, the historical data would not support the allegation that the BLS is biased to either party.\nIf either test reveals statistically significant differences, that would provide empirical support for the claim that CES employment reporting favors Democratic administrations. If both differences are statistically insignificant, the historical data would not support the allegation that the BLS is biased to either party.\n\nTest C: Are CES employment levels significantly higher during Democratic presidencies?\n\n\nCode\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(tidyr)\nlibrary(infer)\nlibrary(ggplot2)\nlibrary(scales)\n\n# 1. Presidential party lookup table (1979–2025)\npresidents_party &lt;- expand_grid(\n  year = 1979:2025,\n  month = month.name,\n  president = NA_character_,\n  party = NA_character_\n) |&gt;\n  mutate(\n    president = case_when(\n      (month == \"January\")  & (year == 1979) ~ \"Carter\",\n      (month == \"February\") & (year == 1981) ~ \"Reagan\",\n      (month == \"February\") & (year == 1989) ~ \"Bush 41\",\n      (month == \"February\") & (year == 1993) ~ \"Clinton\",\n      (month == \"February\") & (year == 2001) ~ \"Bush 43\",\n      (month == \"February\") & (year == 2009) ~ \"Obama\",\n      (month == \"February\") & (year == 2017) ~ \"Trump I\",\n      (month == \"February\") & (year == 2021) ~ \"Biden\",\n      (month == \"February\") & (year == 2025) ~ \"Trump II\",\n      TRUE ~ NA_character_\n    )\n  ) |&gt;\n  tidyr::fill(president) |&gt;\n  mutate(\n    party = if_else(\n      president %in% c(\"Carter\", \"Clinton\", \"Obama\", \"Biden\"),\n      \"D\",\n      \"R\"\n    ),\n    month_num = match(month, month.name)\n  )\n\n# 2. Join CES levels to presidents/parties and build pct_change\n\n\nces_party &lt;- ces_full |&gt;\n  mutate(\n    year = year(date),\n    month_num = month(date)\n  ) |&gt;\n  left_join(\n    presidents_party |&gt;\n      select(year, month_num, president, party),\n    by = c(\"year\", \"month_num\")\n  ) |&gt;\n  filter(!is.na(party)) |&gt;\n  arrange(date) |&gt;\n  mutate(\n    change = level - lag(level),\n    pct_change = change / lag(level)\n  ) |&gt;\n  filter(!is.na(change), !is.na(pct_change))\n\n# 3. Test C: Are monthly percent job gains higher under Democratic presidents?\n\n# Summary table by party (normalized percent changes)\nparty_pct_summary &lt;- ces_party |&gt;\n  group_by(party) |&gt;\n  summarise(\n    Months          = n(),\n    MeanPctChange   = round(mean(pct_change, na.rm = TRUE), 3),\n    MedianPctChange = round(median(pct_change, na.rm = TRUE), 3),\n    SDPCT           = round(sd(pct_change, na.rm = TRUE), 3),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    PartyLabel = if_else(party == \"D\", \"Democrat\", \"Republican\")\n  ) |&gt;\n  select(\n    Party = PartyLabel,\n    Months,\n    MeanPctChange,\n    MedianPctChange,\n    SDPCT\n  )\n\nmake_dt(\n  party_pct_summary,\n  title = \"Monthly Percent Employment Growth by Presidential Party (1979–2025)\",\n  pageLength = 5\n)\n\n\nMonthly Percent Employment Growth by Presidential Party (1979–2025)\n\n\n\n\nCode\n# Welch t-test on normalized percent changes\nparty_pct_test &lt;- t_test(\n  ces_party,\n  pct_change ~ party,\n  order = c(\"R\", \"D\"),    # estimate = Democrat minus Republican\n  alternative = \"greater\" # test if Democrats &gt; Republicans\n)\n\nparty_pct_display &lt;- party_pct_test |&gt;\n  transmute(\n    Difference = round(estimate, 3),\n    T_Df       = round(t_df, 3),\n    P_Value    = signif(p_value, 3),\n    Lower_CI   = round(lower_ci, 3),\n    Upper_CI   = round(upper_ci, 3)\n  )\n\nmake_dt(\n  party_pct_display,\n  title = \"Test C: Are Monthly Percent Job Gains Higher Under Democratic Presidents?\",\n  pageLength = 5\n)\n\n\nTest C: Are Monthly Percent Job Gains Higher Under Democratic Presidents?\n\n\n\n\nCode\npct_diff  &lt;- round(as.numeric(party_pct_test$estimate), 3)\npct_p     &lt;- signif(as.numeric(party_pct_test$p_value), 3)\npct_lower &lt;- round(as.numeric(party_pct_test$lower_ci), 3)\npct_upper &lt;- round(as.numeric(party_pct_test$upper_ci), 3)\n\n# 4. Visual 1: CES employment levels over time colored by party\nggplot(ces_party, aes(x = date, y = level, color = party)) +\n  geom_line() +\n  scale_color_manual(\n    values = c(\"D\" = \"#1F77B4\", \"R\" = \"#D62728\"),\n    labels = c(\"D\" = \"Democrat\", \"R\" = \"Republican\")\n  ) +\n  labs(\n    title = \"CES Employment Levels by Presidential Party\",\n    x = \"Year\",\n    y = \"Employment Level (thousands of jobs)\",\n    color = \"Party\"\n  ) +\n  scale_y_continuous(labels = comma) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text()\n  )\n\n\n\n\n\n\n\n\n\nCode\n# 5. Visual 2: Average monthly percent change by president \npresident_change_summary &lt;- ces_party |&gt;\n  group_by(president, party) |&gt;\n  summarise(\n    MeanPctChange = mean(pct_change, na.rm = TRUE),\n    Months = n(),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    President = factor(\n      president,\n      levels = c(\"Carter\", \"Reagan\", \"Bush 41\", \"Clinton\",\n                 \"Bush 43\", \"Obama\", \"Trump I\", \"Biden\", \"Trump II\")\n    )\n  )\n\nggplot(president_change_summary,\n       aes(x = President, y = MeanPctChange, fill = party)) +\n  geom_col() +\n  scale_fill_manual(\n    values = c(\"D\" = \"#1F77B4\", \"R\" = \"#D62728\"),\n    labels = c(\"D\" = \"Democrat\", \"R\" = \"Republican\")\n  ) +\n  labs(\n    title = \"Average Monthly Percent Job Growth by President\",\n    x = \"President\",\n    y = \"Average Monthly Percent Change\",\n    fill = \"Party\"\n  ) +\n  scale_y_continuous(labels = percent_format(accuracy = 0.01)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\nTest C: Are Monthly Percent Job Gains Higher Under Democratic Presidents? To evaluate the claim that the Bureau of Labor Statistics tends to report stronger job growth under Democratic administrations, we compared the monthly percent change in total nonfarm payroll employment across presidential parties from 1979 to 2025.\nThe hypotheses for this test were:\nH₀: The mean monthly percent job growth is the same under Democratic and Republican presidents.\nH₁: The mean monthly percent job growth is higher under Democratic presidents.\nAcross the full sample, Democratic presidents oversaw 264 months of data with an average monthly percent gain of 0.002, while Republican presidents oversaw 293 months with an average gain of 0.001. The median growth rate was also slightly higher for Democrats (0.002 versus 0.001). However, Republicans had much larger variability in their monthly changes, with a standard deviation of 0.009, compared with 0.002 for Democrats.\nA Welch t-test comparing the distributions found an estimated difference of –0.001 (Democrat minus Republican), with a 95% confidence interval ranging from –0.002 to approximately 0.000. The p-value was 0.956, which is far above the conventional significance threshold of 0.05.\nBecause the p-value is not small and the confidence interval includes zero, we fail to reject the null hypothesis. The data does not provide statistical evidence that monthly percent job growth is higher under Democratic presidents. The small observed difference in averages is well within the range that could be explained by normal variation rather than a systematic pattern related to presidential party.\n\n\nTest D: Are monthly CES revisions more often negative during Democratic presidencies?\n\n\nCode\n# Test D: Are monthly CES revisions more often negative during Democratic presidencies?\n\nlibrary(dplyr)\nlibrary(infer)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(scales)\n\n# ces_party is assumed to contain:\n#   date, level, revision, president, party (\"D\" / \"R\")\n\n# 1. Add a 0/1 indicator for negative revisions\nces_party_neg &lt;- ces_party |&gt;\n  mutate(\n    negative = as.integer(revision &lt; 0)   # 1 = negative revision, 0 = not\n  )\n\n# 2. Summary table: fraction of negative revisions by party (rounded to 3 decimals)\ntd_summary &lt;- ces_party_neg |&gt;\n  group_by(party) |&gt;\n  summarise(\n    Months           = n(),\n    FractionNegative = round(mean(negative, na.rm = TRUE), 3),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    Party = if_else(party == \"D\", \"Democrat\", \"Republican\")\n  ) |&gt;\n  select(\n    Party,\n    Months,\n    FractionNegative\n  )\n\nmake_dt(\n  td_summary,\n  title = \"Fraction of Negative CES Revisions by Presidential Party (1979–2025)\",\n  pageLength = 5\n)\n\n\nFraction of Negative CES Revisions by Presidential Party (1979–2025)\n\n\n\n\nCode\n# 3. Test D: Are negative revisions more common under Democrats?\n#    H0: mean(negative) is the same for D and R\n#    H1: mean(negative) is higher for Democrats\n\ntd_test &lt;- t_test(\n  ces_party_neg,\n  negative ~ party,\n  order = c(\"R\", \"D\"),       # estimate = Democrat minus Republican\n  alternative = \"greater\"    # test if Democrats have more negative revisions\n)\n\ntd_display &lt;- td_test |&gt;\n  transmute(\n    Difference = round(estimate, 3),\n    T_Df       = round(t_df, 3),\n    P_Value    = signif(p_value, 3),\n    Lower_CI   = round(lower_ci, 3),\n    Upper_CI   = round(upper_ci, 3)\n  )\n\nmake_dt(\n  td_display,\n  title = \"Test D: Are Negative Revisions More Common Under Democratic Presidents?\",\n  pageLength = 5\n)\n\n\nTest D: Are Negative Revisions More Common Under Democratic Presidents?\n\n\n\n\nCode\n# Save key values for inline text in your write-up\ntd_diff  &lt;- round(as.numeric(td_test$estimate), 3)\ntd_p     &lt;- signif(as.numeric(td_test$p_value), 3)\ntd_lower &lt;- round(as.numeric(td_test$lower_ci), 3)\ntd_upper &lt;- round(as.numeric(td_test$upper_ci), 3)\n\n# 4. Visual 1: Fraction of negative revisions by president (red = R, blue = D)\n\ntd_pres &lt;- ces_party_neg |&gt;\n  group_by(president, party) |&gt;\n  summarise(\n    Months           = n(),\n    FractionNegative = mean(negative, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    President = factor(\n      president,\n      levels = c(\n        \"Carter\", \"Reagan\", \"Bush 41\", \"Clinton\",\n        \"Bush 43\", \"Obama\", \"Trump I\", \"Biden\", \"Trump II\"\n      )\n    )\n  )\n\nggplot(td_pres,\n       aes(x = President, y = FractionNegative, fill = party)) +\n  geom_col() +\n  scale_fill_manual(\n    values = c(\"D\" = \"#1F77B4\", \"R\" = \"#D62728\"),\n    labels = c(\"D\" = \"Democrat\", \"R\" = \"Republican\")\n  ) +\n  labs(\n    title = \"Share of Negative CES Revisions by President\",\n    x = \"President\",\n    y = \"Fraction of Negative Revisions\",\n    fill = \"Party\"\n  ) +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title  = element_text(face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\nCode\n# 5. Visual 2: Distribution of revisions by party (boxplot, red vs blue)\n\nggplot(ces_party_neg,\n       aes(x = party, y = revision, fill = party)) +\n  geom_boxplot(outlier.alpha = 0.2) +\n  scale_fill_manual(\n    values = c(\"D\" = \"#1F77B4\", \"R\" = \"#D62728\"),\n    labels = c(\"D\" = \"Democrat\", \"R\" = \"Republican\")\n  ) +\n  scale_x_discrete(\n    labels = c(\"D\" = \"Democrat\", \"R\" = \"Republican\")\n  ) +\n  labs(\n    title = \"Distribution of CES Revisions by Presidential Party\",\n    x = \"Party\",\n    y = \"Revision (thousands of jobs)\",\n    fill = \"Party\"\n  ) +\n  scale_y_continuous(labels = comma) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nTest D: Are Negative CES Revisions More Common Under Democratic Presidents?\nTo evaluate the claim that the BLS produces more downward-corrected job numbers when Democrats are in office (meaning their initial bias is to publish higher job numbers during Democrat administrations) we tested whether negative revisions occur more frequently under Democratic presidents.\nThe hypotheses are:\nH₀: The proportion of negative CES revisions is the same under Democratic and Republican presidents.\nH₁: The proportion of negative revisions is higher under Democratic presidents.\nDuring the 1979–2025 period, 36 percent of CES revisions were negative during Democratic presidencies, compared with 48.1 percent during Republican presidencies. This already runs counter to the narrative: negative revisions were actually more common under Republicans.\nThe Welch t-test formalizes this comparison. The estimated difference in proportions (Democrat minus Republican) is: 0.121, meaning Republicans experienced a 12.1-percentage-point higher rate of negative revisions.\nThe 95 percent confidence interval ranges from 0.053 to for now, 1, indicating that the true difference is very likely positive. The p-value is 0.00184, which is far below the 0.05 significance threshold. Because the p-value is extremely small, we reject the null hypothesis. There is strong statistical evidence that negative revisions were not more common under Democrats. In fact, they were significantly less common.\nThese results contradict the fictional commentator’s claim: the historical record shows that CES revisions were more frequently negative under Republican administrations, not Democratic ones."
  }
]